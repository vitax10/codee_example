{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "DG8TYkbnllaON4NtL02d5mzk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG8TYkbnllaON4NtL02d5mzk",
        "outputId": "1c6a90c1-85ed-4ad2-a729-e2a1c64e5589",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !python -m pip uninstall -y dask distributed\n",
        "# !python -m pip install \"dask[distributed]\" --upgrade\n",
        "# !python -m pip install dask --upgrade\n",
        "# !pip install jupyter-server-proxy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea6b43d",
      "metadata": {},
      "source": [
        "## Creation of backtesting subset for NASDAQ-100 DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0lV922CT745M",
      "metadata": {
        "id": "0lV922CT745M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from dask.diagnostics import ProgressBar\n",
        "import os\n",
        "import tempfile\n",
        "from google.cloud import storage\n",
        "import dask\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from dask.distributed import Client, LocalCluster\n",
        "import shutil\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DzLU7yWY76o3",
      "metadata": {
        "id": "DzLU7yWY76o3"
      },
      "outputs": [],
      "source": [
        "# GCS setup\n",
        "BACKTEST_BUCKET = \"backtest_bucket121545\"\n",
        "PROJECT_ID = \"################\"\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Upload a file to a GCS bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    print(f\"Uploaded {source_file_name} to {destination_blob_name}\")\n",
        "\n",
        "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Download a file from a GCS bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    try:\n",
        "        blob.download_to_filename(destination_file_name)\n",
        "        print(f\"Downloaded {source_blob_name} to {destination_file_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {source_blob_name}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_gcs_file_exists(bucket_name, source_blob_name):\n",
        "    \"\"\"Check if a file exists in a GCS bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    return blob.exists()\n",
        "\n",
        "def validate_parquet_file(file_path):\n",
        "    \"\"\"Validate if a file is a valid Parquet file.\"\"\"\n",
        "    try:\n",
        "        pd.read_parquet(file_path, engine='pyarrow')\n",
        "        print(f\"Validated {file_path} as a Parquet file\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"File {file_path} is not a valid Parquet file: {e}\")\n",
        "        return False\n",
        "\n",
        "def upload_files_to_bucket(local_files, destination_folder=\"my_data\"):\n",
        "    \"\"\"\n",
        "    Uploading a list of local files to a GCS bucket under a specified folder and verify the upload.\n",
        "\n",
        "    Args:\n",
        "        local_files (list): List of local file paths to upload.\n",
        "        destination_folder (str): Destination folder in the GCS bucket (default: \"my_data\").\n",
        "\n",
        "    Returns:\n",
        "        list: List of GCS paths for the uploaded files.\n",
        "    \"\"\"\n",
        "    uploaded_files = []\n",
        "\n",
        "    # Check if local files exist\n",
        "    missing_local_files = [file for file in local_files if not os.path.exists(file)]\n",
        "    if missing_local_files:\n",
        "        raise FileNotFoundError(f\"The following local files are missing: {missing_local_files}\")\n",
        "\n",
        "    # Uploading each file and verifying\n",
        "    for local_file in local_files:\n",
        "        destination_blob_name = f\"{destination_folder}/{os.path.basename(local_file)}\"\n",
        "        try:\n",
        "            upload_to_gcs(BACKTEST_BUCKET, local_file, destination_blob_name)\n",
        "            if check_gcs_file_exists(BACKTEST_BUCKET, destination_blob_name):\n",
        "                print(f\"Verified: {destination_blob_name} exists in GCS\")\n",
        "                uploaded_files.append(destination_blob_name)\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to verify upload of {destination_blob_name} in GCS\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to upload {local_file} to {destination_blob_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    return uploaded_files\n",
        "\n",
        "def download_ticker_partition(ticker, prefix=\"parquet_data\", local_dir=\"temp_parquet\"):\n",
        "    \"\"\"\n",
        "    Download all parquet files for a specific ticker partition.\n",
        "\n",
        "    Args:\n",
        "        ticker (str): Ticker symbol to download.\n",
        "        prefix (str): GCS prefix for the parquet data (default: \"parquet_data\").\n",
        "        local_dir (str): Local directory to store downloaded files (default: \"temp_parquet\").\n",
        "\n",
        "    Returns:\n",
        "        list: List of downloaded file paths.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(local_dir):\n",
        "        os.makedirs(local_dir)\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(BACKTEST_BUCKET)\n",
        "    partition_prefix = f\"{prefix}/ticker={ticker}/\"\n",
        "    blobs = bucket.list_blobs(prefix=partition_prefix)\n",
        "\n",
        "    downloaded_files = []\n",
        "    for blob in blobs:\n",
        "        if not blob.name.endswith('.parquet'):\n",
        "            continue\n",
        "        local_file = os.path.join(local_dir, os.path.basename(blob.name))\n",
        "        if download_from_gcs(BACKTEST_BUCKET, blob.name, local_file):\n",
        "            downloaded_files.append(local_file)\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "def batch_download_tickers(tickers, batch_size=10, prefix=\"parquet_data\", local_dir=\"temp_parquet\"):\n",
        "    \"\"\"\n",
        "    Batch download ticker partitions.\n",
        "\n",
        "    Args:\n",
        "        tickers (list): List of ticker symbols to download.\n",
        "        batch_size (int): Number of tickers per batch (default: 10).\n",
        "        prefix (str): GCS prefix for the parquet data (default: \"parquet_data\").\n",
        "        local_dir (str): Local directory to store downloaded files (default: \"temp_parquet\").\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries mapping tickers to their downloaded files.\n",
        "    \"\"\"\n",
        "    total_tickers = len(tickers)\n",
        "    downloaded_batches = []\n",
        "\n",
        "    for batch_start in range(0, total_tickers, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, total_tickers)\n",
        "        batch_tickers = tickers[batch_start:batch_end]\n",
        "        print(f\"Downloading batch: tickers {batch_start + 1} to {batch_end}\")\n",
        "\n",
        "        batch_files = {}\n",
        "        for ticker in batch_tickers:\n",
        "            ticker_files = download_ticker_partition(ticker, prefix, local_dir)\n",
        "            batch_files[ticker] = ticker_files\n",
        "\n",
        "        downloaded_batches.append(batch_files)\n",
        "\n",
        "    return downloaded_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84f1622",
      "metadata": {},
      "source": [
        "### Dask config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_KeJF7W8E4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KeJF7W8E4e",
        "outputId": "88bd2886-5c12-4747-f3f4-3cc652e8c5bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:42395\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42619'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45189'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37385'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35151'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:33113'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38199'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44493'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:32809'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:34249 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:34249\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45968\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:36753 name: 4\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36753\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45980\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46863 name: 5\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46863\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45996\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46377 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46377\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46000\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:36095 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36095\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46004\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:45995 name: 7\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45995\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46018\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:38563 name: 6\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:38563\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46026\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46767 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46767\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46030\n",
            "INFO:distributed.scheduler:Receive client connection: Client-faa0b5ab-24e3-11f0-8243-0242c0a80a0a\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:46044\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dask Dashboard: http://127.0.0.1:8787/status\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<dask.config.set at 0x7b52ca8f2bc0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Progress bar for Dask operations\n",
        "ProgressBar().register()\n",
        "\n",
        "# Configuring Dask spill-to-disk\n",
        "temp_dir = \"/tmp/dask-spill\"  \n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "# Setting up LocalCluster with 8 workers for better parallelism\n",
        "cluster = LocalCluster(\n",
        "    n_workers=8,                     # More workers for 8 vCPUs\n",
        "    threads_per_worker=2,            # 2 threads per worker\n",
        "    memory_limit=\"6GB\",              # ~48GB total\n",
        "    processes=True,                  # Processes for better isolation\n",
        "    local_directory=temp_dir         # Spill to fast disk\n",
        ")\n",
        "client = Client(cluster)\n",
        "print(\"Dask Dashboard:\", client.dashboard_link)\n",
        "\n",
        "dask.config.set({\n",
        "    \"temporary-directory\": temp_dir,\n",
        "    \"distributed.worker.memory.spill\": 0.7,    \n",
        "    \"distributed.worker.memory.target\": 0.6,   \n",
        "    \"distributed.worker.memory.pause\": 0.85,   \n",
        "    \"distributed.worker.memory.terminate\": 0.95,  \n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea3cd928",
      "metadata": {},
      "source": [
        "### Step 1: Loading and filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xasLUhKz8G2M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xasLUhKz8G2M",
        "outputId": "0b3166a2-683f-4a75-8458-e8d53a5f1cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 101 NASDAQ-100 tickers from JSON\n",
            "Filtered to 101 NASDAQ-100 tickers\n",
            "Rows before filtering trading hours: 466547\n",
            "Filtering to trading hours (04:00:00 to 20:00:00, Monday to Friday)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-28c306d075ab>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-5-28c306d075ab>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['hour'] = df['Time'].map(lambda x: x.hour)  # Extract hour from datetime.time\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered to trading hours, rows remaining: 451073\n"
          ]
        }
      ],
      "source": [
        "# Loading NASDAQ-100 tickers from filtered JSON\n",
        "with open('/content/nasdaq100_ticker_dataset.json', 'r') as f:\n",
        "    nasdaq100_data = json.load(f)\n",
        "nasdaq_100 = sorted(set(item['Ticker'] for item in nasdaq100_data if item.get('Ticker')))\n",
        "print(f\"Loaded {len(nasdaq_100)} NASDAQ-100 tickers from JSON\")\n",
        "\n",
        "# Loading the Parquet file\n",
        "file_path = \"backtest_data_step4.parquet\"\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "# Filtering for NASDAQ-100 tickers\n",
        "df = df[df['Ticker'].isin(nasdaq_100)]\n",
        "print(f\"Filtered to {len(df['Ticker'].unique())} NASDAQ-100 tickers\")\n",
        "print(f\"Rows before filtering trading hours: {len(df)}\")\n",
        "\n",
        "# Filtering for trading hours (4:00 AM to 8:00 PM, Monday to Friday)\n",
        "print(\"Filtering to trading hours (04:00:00 to 20:00:00, Monday to Friday)\")\n",
        "df['date'] = pd.to_datetime(df['Date'])\n",
        "df['hour'] = df['Time'].map(lambda x: x.hour)  # Extracting hour from datetime.time\n",
        "df = df[\n",
        "    (df['hour'].between(4, 20)) &  # 4:00 AM to 8:00 PM\n",
        "    (df['date'].dt.weekday < 5)    # Monday to Friday\n",
        "]\n",
        "df = df.drop(columns=['date', 'hour'])  # Drop temporary columns\n",
        "print(f\"Filtered to trading hours, rows remaining: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b442e9d",
      "metadata": {},
      "source": [
        "### Step 2: Creating complete time index - Timestamp column, Deduplicating, Identifying daily bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pQgCHLih8I_s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQgCHLih8I_s",
        "outputId": "408bb81c-2445-45a4-fa48-010d18e182b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Columns after dropping 'Date' and 'Time': ['ticker', 'open', 'high', 'low', 'close', 'volume', 'prev_session_high', 'prev_session_low', 'estimated_bid_ask_spread', 'estimated_obd', '50_day_sma', 'timestamp']\n"
          ]
        }
      ],
      "source": [
        "# Creating a 'timestamp' column by combining 'Date' and 'Time'\n",
        "df['timestamp'] = pd.to_datetime(df['Date']) + pd.to_timedelta(df['Time'].astype(str))\n",
        "\n",
        "# Rename columns\n",
        "df = df.rename(columns={\n",
        "    'Ticker': 'ticker',\n",
        "    'Open': 'open',\n",
        "    'High': 'high',\n",
        "    'Low': 'low',\n",
        "    'Close': 'close',\n",
        "    'Volume': 'volume',\n",
        "    'Prev_Session_High': 'prev_session_high',\n",
        "    'Prev_Session_Low': 'prev_session_low',\n",
        "    'Bid-Ask Spread (Estimated)': 'estimated_bid_ask_spread',\n",
        "    'Estimated OBD': 'estimated_obd',\n",
        "    '50-day SMA': '50_day_sma'\n",
        "})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['Date', 'Time']\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "print(\"Step 1: Columns after dropping 'Date' and 'Time':\", df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ap9kEH7b997j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap9kEH7b997j",
        "outputId": "26c174c8-e311-4c6a-a0ad-261254c17b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Duplicate index entries: MultiIndex([], names=['ticker', 'timestamp'])\n"
          ]
        }
      ],
      "source": [
        "# Deduplicating\n",
        "df = df.set_index(['ticker', 'timestamp'])\n",
        "duplicates = df.index[df.index.duplicated()].unique()\n",
        "print(\"Step 2: Duplicate index entries:\", duplicates)\n",
        "df = df[~df.index.duplicated(keep='last')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aWCzcLCn9_i9",
      "metadata": {
        "id": "aWCzcLCn9_i9"
      },
      "outputs": [],
      "source": [
        "# Identifying daily bounds\n",
        "df_reset = df.reset_index()\n",
        "daily_bounds = df_reset.groupby(['ticker', df_reset['timestamp'].dt.date]).agg({\n",
        "    'timestamp': ['min', 'max']\n",
        "}).reset_index()\n",
        "daily_bounds.columns = ['ticker', 'date', 'first_timestamp', 'last_timestamp']\n",
        "daily_bounds['date'] = pd.to_datetime(daily_bounds['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-l3Xw92w-BS_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l3Xw92w-BS_",
        "outputId": "ac001da5-2a1f-4832-b635-4f204cd6563c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created complete_index_df with 667578 rows\n"
          ]
        }
      ],
      "source": [
        "# Generating complete time index as a DataFrame\n",
        "def generate_time_index(row):\n",
        "    start_time = row['first_timestamp']\n",
        "    end_time = row['last_timestamp']\n",
        "    return pd.date_range(start=start_time, end=end_time, freq='1min')\n",
        "\n",
        "# Creating DataFrame chunks per ticker\n",
        "index_chunks = []\n",
        "for ticker in daily_bounds['ticker'].unique():\n",
        "    ticker_bounds = daily_bounds[daily_bounds['ticker'] == ticker]\n",
        "    time_indices = []\n",
        "    for _, row in ticker_bounds.iterrows():\n",
        "        timestamps = generate_time_index(row)\n",
        "        for ts in timestamps:\n",
        "            time_indices.append((ticker, ts))\n",
        "    ticker_df = pd.DataFrame(time_indices, columns=['ticker', 'timestamp'])\n",
        "    index_chunks.append(ticker_df)\n",
        "\n",
        "# Concatenating into complete_index_df\n",
        "complete_index_df = pd.concat(index_chunks, ignore_index=True)\n",
        "print(\"Created complete_index_df with\", len(complete_index_df), \"rows\")\n",
        "\n",
        "# Optimizing ticker as categorical\n",
        "complete_index_df['ticker'] = complete_index_df['ticker'].astype('category')\n",
        "\n",
        "# Saving\n",
        "complete_index_df.to_parquet('complete_index.parquet', engine='pyarrow', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a52181f",
      "metadata": {},
      "source": [
        "### Step 3: Reindexing and forward-filling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_t3wsoY2-LB-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t3wsoY2-LB-",
        "outputId": "c47db863-9c70-46f5-8c22-5e6d2d7ff901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete index row count: 667578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle c7df865d43083d7aab3b2d43df238ae4 initialized by task ('shuffle-transfer-c7df865d43083d7aab3b2d43df238ae4', 0) executed on worker tcp://127.0.0.1:34249\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle c7df865d43083d7aab3b2d43df238ae4 deactivated due to stimulus 'task-finished-1745922212.9694948'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tickers: 101\n"
          ]
        }
      ],
      "source": [
        "complete_index_ddf = dd.read_parquet('complete_index.parquet', engine='pyarrow')\n",
        "print(\"Complete index row count:\", complete_index_ddf.shape[0].compute())\n",
        "print(\"Unique tickers:\", complete_index_ddf['ticker'].nunique().compute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WMwBp5l5-Lir",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMwBp5l5-Lir",
        "outputId": "ae809c4a-906c-4bfd-cef1-01670ada8480"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5a6e78abc1e6ad91201913ce6c3b5f4c initialized by task ('shuffle-transfer-5a6e78abc1e6ad91201913ce6c3b5f4c', 0) executed on worker tcp://127.0.0.1:34249\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5a6e78abc1e6ad91201913ce6c3b5f4c deactivated due to stimulus 'task-finished-1745922213.7761288'\n"
          ]
        }
      ],
      "source": [
        "# Converting 'ticker' column to ordered categorical type\n",
        "complete_index_ddf['ticker'] = complete_index_ddf['ticker'].astype(CategoricalDtype(categories=complete_index_ddf['ticker'].unique().compute(), ordered=True))\n",
        "\n",
        "# Sorting and repartitioning by ticker\n",
        "complete_index_ddf = complete_index_ddf.sort_values(['ticker', 'timestamp']).repartition(partition_size=\"100MB\").persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xPOIGj_X-PhD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPOIGj_X-PhD",
        "outputId": "dc4d8451-de93-42c6-926d-3fbb23449edc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5472c52eef522566dd798bd4af4d66ba initialized by task ('shuffle-transfer-5472c52eef522566dd798bd4af4d66ba', 0) executed on worker tcp://127.0.0.1:34249\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5: Starting reindex and forward-fill process\n",
            "Step 5: Columns before converting to Dask: ['ticker', 'timestamp', 'open', 'high', 'low', 'close', 'volume', 'prev_session_high', 'prev_session_low', 'estimated_bid_ask_spread', 'estimated_obd', '50_day_sma']\n",
            "Step 5: Ensuring consistent ticker categories\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5472c52eef522566dd798bd4af4d66ba deactivated due to stimulus 'task-finished-1745922214.8882167'\n",
            "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 38.29 MiB.\n",
            "This may cause some slowdown.\n",
            "Consider loading the data with Dask directly\n",
            " or using futures or delayed objects to embed the data into the graph without repetition.\n",
            "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5: Casting numeric columns to float64\n",
            "Step 5: Converting Pandas DataFrame to Dask DataFrame\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 76.58 MiB.\n",
            "This may cause some slowdown.\n",
            "Consider loading the data with Dask directly\n",
            " or using futures or delayed objects to embed the data into the graph without repetition.\n",
            "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5: Performing left merge with complete index\n",
            "Step 5: Original row count: 451073\n",
            "Step 5: Complete index row count: 667578\n",
            "Step 5: Merged row count: 667578\n",
            "Step 5: Columns after merge: ['ticker', 'timestamp', 'open', 'high', 'low', 'close', 'volume', 'prev_session_high', 'prev_session_low', 'estimated_bid_ask_spread', 'estimated_obd', '50_day_sma']\n",
            "Step 5: Missing data percentages after merge:\n",
            "  ticker: 0.00%\n",
            "  timestamp: 0.00%\n",
            "  open: 32.43%\n",
            "  high: 32.43%\n",
            "  low: 32.43%\n",
            "  close: 32.43%\n",
            "  volume: 32.43%\n",
            "  prev_session_high: 32.93%\n",
            "  prev_session_low: 32.93%\n",
            "  estimated_bid_ask_spread: 32.43%\n",
            "  estimated_obd: 32.43%\n",
            "  50_day_sma: 32.43%\n",
            "Step 5: Sorting by ticker and timestamp\n",
            "Step 5: Performing forward-fill within ticker-date groups\n",
            "Step 5: Defining session masks for regular and extended hours\n",
            "Step 5: Interpolating volume for regular hours\n",
            "Step 5: Setting volume to small value for extended hours\n",
            "Step 5: Handling volume for non-trading hours\n",
            "Step 5: Combining regular, extended, and non-trading hours data\n",
            "Step 5: Applying final fill for any remaining missing volume\n",
            "Step 5: Dropping rows with missing ticker or timestamp\n",
            "Step 5: Dropping temporary columns\n",
            "Step 5: Calculating missing percentages for each column\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff initialized by task ('shuffle-transfer-6e2cbfb260c87eb611a20fd5198f1fff', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff deactivated due to stimulus 'task-finished-1745922218.0816333'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff initialized by task ('shuffle-transfer-6e2cbfb260c87eb611a20fd5198f1fff', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff deactivated due to stimulus 'task-finished-1745922220.2930238'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle b1037448a5eae990b299e6c268b30dab initialized by task ('shuffle-transfer-b1037448a5eae990b299e6c268b30dab', 2) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle b1037448a5eae990b299e6c268b30dab deactivated due to stimulus 'task-finished-1745922221.783676'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing percentages (%):\n",
            "  ticker: 0.00%\n",
            "  timestamp: 0.00%\n",
            "  open: 0.00%\n",
            "  high: 0.00%\n",
            "  low: 0.00%\n",
            "  close: 0.00%\n",
            "  volume: 0.00%\n",
            "  prev_session_high: 0.61%\n",
            "  prev_session_low: 0.61%\n",
            "  estimated_bid_ask_spread: 32.43%\n",
            "  estimated_obd: 32.43%\n",
            "  50_day_sma: 0.00%\n",
            "Step 5: Defining PyArrow schema for parquet output\n"
          ]
        }
      ],
      "source": [
        "# Reindexing and forward-fill using Dask\n",
        "print(df_reset.columns.tolist())\n",
        "\n",
        "# Consistent ticker categories\n",
        "unique_tickers = pd.concat([complete_index_ddf['ticker'].unique().compute(),\n",
        "                           pd.Series(df_reset['ticker'].unique())]).unique()\n",
        "ticker_categories = CategoricalDtype(categories=unique_tickers, ordered=True)\n",
        "df_reset['ticker'] = df_reset['ticker'].astype(ticker_categories)\n",
        "complete_index_ddf['ticker'] = complete_index_ddf['ticker'].astype(ticker_categories)\n",
        "\n",
        "# Numeric columns to float64\n",
        "numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'prev_session_high',\n",
        "                'prev_session_low', 'estimated_bid_ask_spread', 'estimated_obd', '50_day_sma']\n",
        "df_reset[numeric_cols] = df_reset[numeric_cols].astype('float64', errors='ignore')\n",
        "\n",
        "# Converting to Dask DataFrame with optimized partitions\n",
        "ddf = dd.from_pandas(df_reset, npartitions=50).repartition(partition_size=\"100MB\").persist()\n",
        "\n",
        "# Merging with complete index - left merge\n",
        "print(\"Original row count:\", ddf.shape[0].compute())\n",
        "print(\"Complete index row count:\", complete_index_ddf.shape[0].compute())\n",
        "ddf = complete_index_ddf.merge(ddf, on=['ticker', 'timestamp'], how='left').persist()\n",
        "print(\"Merged row count:\", ddf.shape[0].compute())\n",
        "print(\"Columns after merge:\", ddf.columns.tolist())\n",
        "\n",
        "# Missing percentages after merge\n",
        "missing_percentages = ddf.isna().mean().compute() * 100\n",
        "for col, percentage in missing_percentages.items():\n",
        "    print(f\"  {col}: {percentage:.2f}%\")\n",
        "\n",
        "# Sorting and forward-fill\n",
        "ddf = ddf.sort_values(['ticker', 'timestamp']).persist()\n",
        "ddf['date_only'] = ddf['timestamp'].dt.date\n",
        "cols_to_ffill = ['open', 'high', 'low', 'close', 'prev_session_high', 'prev_session_low', '50_day_sma']\n",
        "ddf[cols_to_ffill] = ddf.groupby(['ticker', 'date_only'], observed=True)[cols_to_ffill].ffill()\n",
        "ddf = ddf.reset_index(drop=True)\n",
        "\n",
        "# Session masks for volume handling\n",
        "ddf['hour'] = ddf['timestamp'].dt.hour\n",
        "ddf['minute'] = ddf['timestamp'].dt.minute\n",
        "ddf['is_regular'] = ((ddf['hour'] == 9) & (ddf['minute'] >= 30)) | (ddf['hour'].between(10, 15)) | ((ddf['hour'] == 16) & (ddf['minute'] == 0))\n",
        "ddf['is_extended'] = (ddf['hour'].between(4, 20)) & (~ddf['is_regular'])\n",
        "\n",
        "# Volume handling functions\n",
        "def interpolate_volume_regular_hours(df):\n",
        "    df['volume'] = df['volume'].interpolate(method='linear', limit_direction='both')  \n",
        "    df['volume'] = df['volume'].clip(lower=0)\n",
        "    return df\n",
        "\n",
        "def fill_volume_extended_hours(df):\n",
        "    df['volume'] = df['volume'].fillna(0.0000000001)  # Using small value instead of 0\n",
        "    return df\n",
        "\n",
        "# Processing volume for regular and extended hours - Interpolation and small value fill\n",
        "regular_ddf = ddf[ddf['is_regular']].copy()\n",
        "regular_ddf = regular_ddf.map_partitions(interpolate_volume_regular_hours, meta=ddf)\n",
        "\n",
        "extended_ddf = ddf[ddf['is_extended']].copy()\n",
        "extended_ddf = extended_ddf.map_partitions(fill_volume_extended_hours, meta=ddf)\n",
        "\n",
        "# Handling non-trading hours\n",
        "non_trading_ddf = ddf[~(ddf['is_regular'] | ddf['is_extended'])].copy()\n",
        "non_trading_ddf['volume'] = non_trading_ddf['volume'].fillna(0.0000000001)  # small value\n",
        "\n",
        "# Combining and sorting\n",
        "print(\"Step 5: Combining regular, extended, and non-trading hours data\")\n",
        "ddf = dd.concat([regular_ddf, extended_ddf, non_trading_ddf]).sort_values(['ticker', 'timestamp'])\n",
        "\n",
        "# Final fill to catch any remaining NaN in volume\n",
        "print(\"Step 5: Applying final fill for any remaining missing volume\")\n",
        "ddf['volume'] = ddf['volume'].fillna(0.0000000001)\n",
        "\n",
        "# Cleaning\n",
        "ddf = ddf.dropna(subset=['ticker', 'timestamp'])\n",
        "ddf = ddf.drop(columns=['date_only', 'hour', 'minute', 'is_regular', 'is_extended'])\n",
        "\n",
        "# Missing percentages\n",
        "missing_percentages = ddf.isna().mean().compute() * 100\n",
        "for column, percentage in missing_percentages.items():\n",
        "    print(f\"  {column}: {percentage:.2f}%\")\n",
        "\n",
        "# PyArrow schema\n",
        "schema_forward = pa.schema([\n",
        "    ('ticker', pa.dictionary(pa.int16(), pa.string())),\n",
        "    ('timestamp', pa.timestamp('ns')),\n",
        "    ('open', pa.float64()),\n",
        "    ('high', pa.float64()),\n",
        "    ('low', pa.float64()),\n",
        "    ('close', pa.float64()),\n",
        "    ('volume', pa.float64()),\n",
        "    ('prev_session_high', pa.float64()),\n",
        "    ('prev_session_low', pa.float64()),\n",
        "    ('estimated_bid_ask_spread', pa.float64()),\n",
        "    ('estimated_obd', pa.float64()),\n",
        "    ('50_day_sma', pa.float64())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721a4666",
      "metadata": {},
      "source": [
        "### Step 4: Computing prev_session_high/low, backward-fill and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7StHMbcl-YN1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7StHMbcl-YN1",
        "outputId": "0caeba6e-1a67-4367-9bad-a8914ce75cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 6: Starting computation of prev_session_high/low, backward-fill, and metrics\n",
            "Step 6: Optimizing data types to float32\n",
            "Step 6: Sorting by ticker and timestamp\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff initialized by task ('shuffle-transfer-6e2cbfb260c87eb611a20fd5198f1fff', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff deactivated due to stimulus 'task-finished-1745922223.1384077'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff initialized by task ('shuffle-transfer-6e2cbfb260c87eb611a20fd5198f1fff', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 6e2cbfb260c87eb611a20fd5198f1fff deactivated due to stimulus 'task-finished-1745922225.0588744'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 7c4b50f09d6c270e7fa1ee1258d44824 initialized by task ('shuffle-transfer-7c4b50f09d6c270e7fa1ee1258d44824', 2) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 7c4b50f09d6c270e7fa1ee1258d44824 deactivated due to stimulus 'task-finished-1745922226.226569'\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 6: Computing daily high and low per ticker\n",
            "Step 6: Shifting to get previous trading day's values\n",
            "Step 6: Forward-filling missing previous trading dates and values\n",
            "Step 6: Merging computed prev_session_high/low into main DataFrame\n",
            "Step 6: Filling missing prev_session_high/low with computed values\n",
            "Step 6: Dropping temporary computed columns\n",
            "Step 6: Calculating missing percentages for each column\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5be5c978223fa5fa3a47b2e49b17e8e6 initialized by task ('shuffle-transfer-5be5c978223fa5fa3a47b2e49b17e8e6', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5be5c978223fa5fa3a47b2e49b17e8e6 deactivated due to stimulus 'task-finished-1745922350.9722843'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle f9f2fcd7246238989f0333f365010125 initialized by task ('shuffle-transfer-f9f2fcd7246238989f0333f365010125', 2) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle f9f2fcd7246238989f0333f365010125 deactivated due to stimulus 'task-finished-1745922352.3017833'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff initialized by task ('shuffle-transfer-3ed8c5512bba53dbb5e97235e52fd0ff', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 initialized by task ('shuffle-transfer-9a2586511967bbdb31b967b086449ad6', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff deactivated due to stimulus 'task-finished-1745922352.7524238'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 deactivated due to stimulus 'task-finished-1745922352.7531054'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba initialized by task ('shuffle-transfer-ebee76d45fe2d1f458f3db373edd77ba', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba deactivated due to stimulus 'task-finished-1745922352.878219'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd initialized by task ('shuffle-transfer-5e7e326ebaff5b96c26873a69fd9c6fd', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 initialized by task ('shuffle-transfer-d8e19273e9004b432ef832e45f3495b1', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd deactivated due to stimulus 'task-finished-1745922352.9589543'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 deactivated due to stimulus 'task-finished-1745922352.9646692'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 initialized by task ('shuffle-transfer-0034d1cc83c1cd902208853b6a355222', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 deactivated due to stimulus 'task-finished-1745922353.0418854'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing percentages (%):\n",
            "  ticker: 0.00%\n",
            "  timestamp: 0.00%\n",
            "  open: 0.00%\n",
            "  high: 0.00%\n",
            "  low: 0.00%\n",
            "  close: 0.00%\n",
            "  volume: 0.00%\n",
            "  prev_session_high: 0.58%\n",
            "  prev_session_low: 0.58%\n",
            "  estimated_bid_ask_spread: 32.43%\n",
            "  estimated_obd: 32.43%\n",
            "  50_day_sma: 0.00%\n",
            "  date_only: 0.00%\n",
            "Step 6: Handling remaining missing prev_session_high/low\n",
            "Step 6: Performing backward-fill for specified columns\n",
            "Step 6: Computing estimated bid-ask spread and order book depth\n",
            "Step 6: Dropping temporary date_only column\n",
            "Step 6: Calculating missing percentages for each column\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 initialized by task ('shuffle-transfer-9a2586511967bbdb31b967b086449ad6', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff initialized by task ('shuffle-transfer-3ed8c5512bba53dbb5e97235e52fd0ff', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff deactivated due to stimulus 'task-finished-1745922354.334429'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 deactivated due to stimulus 'task-finished-1745922354.3382974'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba initialized by task ('shuffle-transfer-ebee76d45fe2d1f458f3db373edd77ba', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba deactivated due to stimulus 'task-finished-1745922354.4016345'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd initialized by task ('shuffle-transfer-5e7e326ebaff5b96c26873a69fd9c6fd', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 initialized by task ('shuffle-transfer-d8e19273e9004b432ef832e45f3495b1', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd deactivated due to stimulus 'task-finished-1745922354.477665'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 deactivated due to stimulus 'task-finished-1745922354.483085'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 initialized by task ('shuffle-transfer-0034d1cc83c1cd902208853b6a355222', 0) executed on worker tcp://127.0.0.1:36753\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 deactivated due to stimulus 'task-finished-1745922354.5610604'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing percentages (%):\n",
            "  ticker: 0.00%\n",
            "  timestamp: 0.00%\n",
            "  open: 0.00%\n",
            "  high: 0.00%\n",
            "  low: 0.00%\n",
            "  close: 0.00%\n",
            "  volume: 0.00%\n",
            "  prev_session_high: 0.00%\n",
            "  prev_session_low: 0.00%\n",
            "  estimated_bid_ask_spread: 0.00%\n",
            "  estimated_obd: 0.00%\n",
            "  50_day_sma: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# Optimizing data types once again\n",
        "ddf['open'] = ddf['open'].astype('float32')\n",
        "ddf['high'] = ddf['high'].astype('float32')\n",
        "ddf['low'] = ddf['low'].astype('float32')\n",
        "ddf['close'] = ddf['close'].astype('float32')\n",
        "ddf['volume'] = ddf['volume'].astype('float32')\n",
        "ddf['50_day_sma'] = ddf['50_day_sma'].astype('float32')\n",
        "\n",
        "# Sorting\n",
        "ddf = ddf.sort_values(['ticker', 'timestamp']).persist()\n",
        "\n",
        "# Computing daily high and low per ticker\n",
        "daily_high_low = ddf.groupby(['ticker', ddf['timestamp'].dt.date], observed=False).agg({\n",
        "    'high': 'max',\n",
        "    'low': 'min'\n",
        "}).reset_index()\n",
        "daily_high_low = daily_high_low.rename(columns={\n",
        "    'timestamp': 'date_only',\n",
        "    'high': 'prev_session_high',\n",
        "    'low': 'prev_session_low'\n",
        "})\n",
        "\n",
        "# Shift to get previous trading day's values\n",
        "meta_shift = {\n",
        "    'ticker': 'category',\n",
        "    'date_only': 'object',\n",
        "    'prev_session_high': 'float32',\n",
        "    'prev_session_low': 'float32',\n",
        "    'prev_trading_date': 'object'\n",
        "}\n",
        "daily_high_low['prev_trading_date'] = daily_high_low.groupby('ticker', observed=False)['date_only'].shift(1, meta=('prev_trading_date', 'object'))\n",
        "daily_high_low['prev_session_high'] = daily_high_low.groupby('ticker', observed=False)['prev_session_high'].shift(1, meta=('prev_session_high', 'float32'))\n",
        "daily_high_low['prev_session_low'] = daily_high_low.groupby('ticker', observed=False)['prev_session_low'].shift(1, meta=('prev_session_low', 'float32'))\n",
        "\n",
        "# Forward-filling missing values\n",
        "daily_high_low['prev_trading_date'] = daily_high_low.groupby('ticker', observed=False)['prev_trading_date'].ffill()\n",
        "daily_high_low['prev_session_high'] = daily_high_low.groupby('ticker', observed=False)['prev_session_high'].ffill()\n",
        "daily_high_low['prev_session_low'] = daily_high_low.groupby('ticker', observed=False)['prev_session_low'].ffill()\n",
        "\n",
        "# Merging computed prev_session_high/low\n",
        "ddf['date_only'] = ddf['timestamp'].dt.date\n",
        "ddf = ddf.merge(\n",
        "    daily_high_low[['ticker', 'date_only', 'prev_session_high', 'prev_session_low']],\n",
        "    left_on=['ticker', 'date_only'],\n",
        "    right_on=['ticker', 'date_only'],\n",
        "    how='left',\n",
        "    suffixes=('', '_computed')\n",
        ")\n",
        "\n",
        "# Filling missing prev_session_high/low\n",
        "ddf['prev_session_high'] = ddf['prev_session_high'].where(\n",
        "    ~ddf['prev_session_high'].isna(), ddf['prev_session_high_computed']\n",
        ")\n",
        "ddf['prev_session_low'] = ddf['prev_session_low'].where(\n",
        "    ~ddf['prev_session_low'].isna(), ddf['prev_session_low_computed']\n",
        ")\n",
        "\n",
        "# Dropping temp\n",
        "ddf = ddf.drop(columns=['prev_session_high_computed', 'prev_session_low_computed'])\n",
        "\n",
        "# Missing percentages\n",
        "missing_percentages = ddf.isna().mean().compute() * 100\n",
        "print(\"Missing percentages (%):\")\n",
        "for column, percentage in missing_percentages.items():\n",
        "    print(f\"  {column}: {percentage:.2f}%\")\n",
        "\n",
        "# Handling remaining missing prev_session_high/low\n",
        "ddf['prev_session_high'] = ddf['prev_session_high'].fillna(ddf['high'])\n",
        "ddf['prev_session_low'] = ddf['prev_session_low'].fillna(ddf['low'])\n",
        "\n",
        "# Backward-filling\n",
        "meta_bfill = {\n",
        "    'ticker': 'category',\n",
        "    'timestamp': 'datetime64[ns]',\n",
        "    'open': 'float32',\n",
        "    'high': 'float32',\n",
        "    'low': 'float32',\n",
        "    'close': 'float32',\n",
        "    'volume': 'float32',\n",
        "    'prev_session_high': 'float32',\n",
        "    'prev_session_low': 'float32',\n",
        "    'estimated_bid_ask_spread': 'float32',\n",
        "    'estimated_obd': 'float32',\n",
        "    '50_day_sma': 'float32',\n",
        "    'date_only': 'object'\n",
        "}\n",
        "\n",
        "def bfill_partition(partition):\n",
        "    partition = partition.set_index(['ticker', 'timestamp'])\n",
        "    partition['date_only'] = partition.index.get_level_values('timestamp').date\n",
        "    cols_to_bfill = ['open', 'high', 'low', 'close', '50_day_sma']\n",
        "    partition[cols_to_bfill] = partition.groupby(['ticker', 'date_only'], observed=True)[cols_to_bfill].bfill()\n",
        "    return partition.reset_index().drop(columns=['index'], errors='ignore')\n",
        "\n",
        "ddf = ddf.map_partitions(bfill_partition, meta=meta_bfill)\n",
        "\n",
        "# Computing metrics - estimated bid-ask spread and obd\n",
        "def compute_metrics_partition(partition):\n",
        "    partition['estimated_bid_ask_spread'] = partition['high'] - partition['low']\n",
        "    partition['estimated_bid_ask_spread'] = partition['estimated_bid_ask_spread'].clip(lower=0.0001)\n",
        "    partition['estimated_obd'] = partition['volume'] / partition['estimated_bid_ask_spread']\n",
        "    return partition\n",
        "\n",
        "ddf = ddf.map_partitions(compute_metrics_partition, meta=meta_bfill)\n",
        "\n",
        "# Drop temp\n",
        "ddf = ddf.drop(columns=['date_only'])\n",
        "\n",
        "missing_percentages = ddf.isna().mean().compute() * 100\n",
        "print(\"Missing percentages (%):\")\n",
        "for column, percentage in missing_percentages.items():\n",
        "    print(f\"  {column}: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "355978ae",
      "metadata": {},
      "source": [
        "### Step 5: Adding News impact binary indicator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rs0bOVlh-fRB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs0bOVlh-fRB",
        "outputId": "62d1e183-fa05-4891-b31a-e80b3b447fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 7: Adding news impact\n"
          ]
        }
      ],
      "source": [
        "news_df = pd.read_csv('final_news.csv')\n",
        "\n",
        "# Converting Impact_Date to datetime (date only)\n",
        "news_df['Impact_Date'] = pd.to_datetime(news_df['Impact_Date']).dt.date\n",
        "news_df['Date'] = pd.to_datetime(news_df['Date']).dt.date\n",
        "\n",
        "# Filtering news for NASDAQ-100 tickers\n",
        "news_df = news_df[news_df['Ticker'].isin(nasdaq_100) | news_df['Ticker'].isna()]\n",
        "\n",
        "# Handling Ticker column\n",
        "news_df['Ticker'] = news_df['Ticker'].replace('', None).replace('None', None)\n",
        "\n",
        "# Splitting into general and ticker-specific news\n",
        "general_news = news_df[news_df['Ticker'].isna()][['Impact_Date']].drop_duplicates()\n",
        "ticker_news = news_df[news_df['Ticker'].notna()][['Ticker', 'Impact_Date']].drop_duplicates()\n",
        "\n",
        "# Converting to sets for efficient lookup\n",
        "general_news_dates = set(general_news['Impact_Date'])\n",
        "ticker_news_pairs = set(ticker_news.apply(lambda row: (row['Ticker'], row['Impact_Date']), axis=1))\n",
        "\n",
        "# Adding news_impact column\n",
        "def add_news_impact(partition):\n",
        "    partition['date_only'] = partition['timestamp'].dt.date\n",
        "    partition['news_impact'] = 0\n",
        "    partition['news_impact'] = partition['news_impact'].where(\n",
        "        ~partition['date_only'].isin(general_news_dates), 1\n",
        "    )\n",
        "    partition['ticker_date'] = partition.apply(\n",
        "        lambda row: (row['ticker'], row['date_only']), axis=1\n",
        "    )\n",
        "    partition['news_impact'] = partition['news_impact'].where(\n",
        "        ~partition['ticker_date'].isin(ticker_news_pairs), 1\n",
        "    )\n",
        "    partition = partition.drop(columns=['date_only', 'ticker_date'])\n",
        "    return partition\n",
        "\n",
        "meta_with_news = {\n",
        "    'ticker': 'category',\n",
        "    'timestamp': 'datetime64[ns]',\n",
        "    'open': 'float64',\n",
        "    'high': 'float64',\n",
        "    'low': 'float64',\n",
        "    'close': 'float64',\n",
        "    'volume': 'float64',\n",
        "    'prev_session_high': 'float64',\n",
        "    'prev_session_low': 'float64',\n",
        "    'estimated_bid_ask_spread': 'float64',\n",
        "    'estimated_obd': 'float64',\n",
        "    '50_day_sma': 'float64',\n",
        "    'news_impact': 'int8'\n",
        "}\n",
        "\n",
        "ddf = ddf.map_partitions(add_news_impact, meta=meta_with_news)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8f4eb1",
      "metadata": {},
      "source": [
        "### Step 6: Finalizing and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ki5VjDBRS4F4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki5VjDBRS4F4",
        "outputId": "c1813b97-ce4a-4cb4-9ac1-281c7f7ad81a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 8: Finalizing DataFrame\n",
            "Step 8»; Repartitioning and sorting DataFrame\n"
          ]
        }
      ],
      "source": [
        "# Finalizing dataframe\n",
        "def finalize_partition(partition):\n",
        "    final_columns = [\n",
        "        'ticker', 'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
        "        'prev_session_high', 'prev_session_low', 'estimated_bid_ask_spread',\n",
        "        'estimated_obd', '50_day_sma', 'news_impact'\n",
        "    ]\n",
        "    partition = partition.drop_duplicates(subset=final_columns, keep='first')\n",
        "    return partition[final_columns]\n",
        "\n",
        "meta_final = {\n",
        "    'ticker': 'category',\n",
        "    'timestamp': 'datetime64[ns]',\n",
        "    'open': 'float64',\n",
        "    'high': 'float64',\n",
        "    'low': 'float64',\n",
        "    'close': 'float64',\n",
        "    'volume': 'float64',\n",
        "    'prev_session_high': 'float64',\n",
        "    'prev_session_low': 'float64',\n",
        "    'estimated_bid_ask_spread': 'float64',\n",
        "    'estimated_obd': 'float64',\n",
        "    '50_day_sma': 'float64',\n",
        "    'news_impact': 'int8'\n",
        "}\n",
        "\n",
        "ddf = ddf.map_partitions(finalize_partition, meta=meta_final)\n",
        "\n",
        "print(\"Row count:\", ddf.shape[0].compute())\n",
        "\n",
        "#ddf = ddf.repartition(partition_size=\"50MB\").persist()\n",
        "#ddf = ddf.sort_values(['ticker', 'timestamp']).persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rr82JeH1P3PZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr82JeH1P3PZ",
        "outputId": "ed0952e9-ab3c-4a3b-850c-edd49a2372aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 9: Saving to Parquet\n",
            "Number of unique tickers: 101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff initialized by task ('shuffle-transfer-3ed8c5512bba53dbb5e97235e52fd0ff', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 initialized by task ('shuffle-transfer-9a2586511967bbdb31b967b086449ad6', 0) executed on worker tcp://127.0.0.1:46863\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 3ed8c5512bba53dbb5e97235e52fd0ff deactivated due to stimulus 'task-finished-1745922400.0215952'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 9a2586511967bbdb31b967b086449ad6 deactivated due to stimulus 'task-finished-1745922400.0276268'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba initialized by task ('shuffle-transfer-ebee76d45fe2d1f458f3db373edd77ba', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle ebee76d45fe2d1f458f3db373edd77ba deactivated due to stimulus 'task-finished-1745922400.086595'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd initialized by task ('shuffle-transfer-5e7e326ebaff5b96c26873a69fd9c6fd', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 initialized by task ('shuffle-transfer-d8e19273e9004b432ef832e45f3495b1', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 5e7e326ebaff5b96c26873a69fd9c6fd deactivated due to stimulus 'task-finished-1745922400.1604278'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle d8e19273e9004b432ef832e45f3495b1 deactivated due to stimulus 'task-finished-1745922400.1667595'\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 initialized by task ('shuffle-transfer-0034d1cc83c1cd902208853b6a355222', 0) executed on worker tcp://127.0.0.1:38563\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 0034d1cc83c1cd902208853b6a355222 deactivated due to stimulus 'task-finished-1745922400.2392883'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to save Parquet to /content/backtesting_final: 4.14 seconds\n"
          ]
        }
      ],
      "source": [
        "# Saving to parquet\n",
        "unique_tickers = sorted(nasdaq_100)\n",
        "print(f\"Number of unique tickers: {len(unique_tickers)}\")\n",
        "\n",
        "\n",
        "# Output directory\n",
        "output_dir_final = '/content/backtesting_final'\n",
        "os.makedirs(output_dir_final, exist_ok=True)\n",
        "\n",
        "# PyArrow schema\n",
        "schema_forward = pa.schema([\n",
        "    ('ticker', pa.dictionary(pa.int16(), pa.string(), ordered=True)),  # Specifying ordered=True\n",
        "    ('timestamp', pa.timestamp('ns')),\n",
        "    ('open', pa.float32()),\n",
        "    ('high', pa.float32()),\n",
        "    ('low', pa.float32()),\n",
        "    ('close', pa.float32()),\n",
        "    ('volume', pa.float32()),\n",
        "    ('prev_session_high', pa.float32()),\n",
        "    ('prev_session_low', pa.float32()),\n",
        "    ('estimated_bid_ask_spread', pa.float32()),\n",
        "    ('estimated_obd', pa.float32()),\n",
        "    ('50_day_sma', pa.float32()),\n",
        "    ('news_impact', pa.int8())\n",
        "])\n",
        "\n",
        "# Writing to Parquet with partitioning by ticker\n",
        "start_time = time.time()\n",
        "ddf.to_parquet(\n",
        "    output_dir_final,\n",
        "    engine='pyarrow',\n",
        "    schema=schema_forward,\n",
        "    partition_on=['ticker'],  # Partitioning by ticker column\n",
        "    write_metadata_file=False,\n",
        "    compression=None,  # mAKING it faster without compression\n",
        "    append=False\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Time to save Parquet to {output_dir_final}: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae24e8",
      "metadata": {},
      "source": [
        "### AAPL inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Ltyp9bMB8lX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ltyp9bMB8lX",
        "outputId": "718a587f-c30b-42a0-81cf-79975699dd07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL data saved to aapl_data.csv\n",
            "Number of rows: 3809\n",
            "Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'prev_session_high', 'prev_session_low', 'estimated_bid_ask_spread', 'estimated_obd', '50_day_sma', 'news_impact', 'ticker']\n",
            "First 5 rows for verification:\n",
            "            timestamp       open       high        low      close  \\\n",
            "0 2020-03-06 09:00:00  72.250000  72.250000  72.004997  72.004997   \n",
            "1 2020-03-06 09:01:00  72.025002  72.025002  72.025002  72.025002   \n",
            "2 2020-03-06 09:02:00  72.025002  72.025002  72.025002  72.025002   \n",
            "3 2020-03-06 09:03:00  72.025002  72.025002  72.025002  72.025002   \n",
            "4 2020-03-06 09:04:00  72.232498  72.250000  72.232498  72.250000   \n",
            "\n",
            "         volume  prev_session_high  prev_session_low  \\\n",
            "0  2.880000e+03          74.887497         72.852501   \n",
            "1  4.532000e+03          74.887497         72.852501   \n",
            "2  1.000000e-10          74.887497         72.852501   \n",
            "3  1.000000e-10          74.887497         72.852501   \n",
            "4  5.668000e+03          74.887497         72.852501   \n",
            "\n",
            "   estimated_bid_ask_spread  estimated_obd  50_day_sma  news_impact ticker  \n",
            "0                  0.245003   1.175497e+04   71.730003            1   AAPL  \n",
            "1                  0.000100   4.532000e+07   71.730003            1   AAPL  \n",
            "2                  0.000100   1.000000e-06   71.730003            1   AAPL  \n",
            "3                  0.000100   1.000000e-06   71.730003            1   AAPL  \n",
            "4                  0.017502   3.238518e+05   71.730003            1   AAPL  \n"
          ]
        }
      ],
      "source": [
        "# Path\n",
        "parquet_path = \"/content/backtesting_final/\"\n",
        "\n",
        "# path - AAPL subfolder\n",
        "aapl_subfolder = os.path.join(parquet_path, \"ticker=AAPL\")\n",
        "if not os.path.exists(aapl_subfolder):\n",
        "    raise FileNotFoundError(f\"Subfolder for ticker=AAPL not found at {aapl_subfolder}\")\n",
        "\n",
        "# All Parquet files in the AAPL subfolder\n",
        "parquet_files = glob.glob(os.path.join(aapl_subfolder, \"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    raise FileNotFoundError(f\"No Parquet files found in {aapl_subfolder}\")\n",
        "\n",
        "# Reading all Parquet files into a list of DataFrames\n",
        "tables = [pq.read_table(file) for file in parquet_files]\n",
        "\n",
        "# Into single table\n",
        "table = pa.concat_tables(tables)\n",
        "\n",
        "df_aapl = table.to_pandas()\n",
        "\n",
        "# Correct data types based on my schema\n",
        "df_aapl['ticker'] = df_aapl['ticker'].astype('category')\n",
        "df_aapl['timestamp'] = pd.to_datetime(df_aapl['timestamp'])\n",
        "df_aapl['open'] = df_aapl['open'].astype('float64')\n",
        "df_aapl['high'] = df_aapl['high'].astype('float64')\n",
        "df_aapl['low'] = df_aapl['low'].astype('float64')\n",
        "df_aapl['close'] = df_aapl['close'].astype('float64')\n",
        "df_aapl['volume'] = df_aapl['volume'].astype('float64')\n",
        "df_aapl['prev_session_high'] = df_aapl['prev_session_high'].astype('float64')\n",
        "df_aapl['prev_session_low'] = df_aapl['prev_session_low'].astype('float64')\n",
        "df_aapl['estimated_bid_ask_spread'] = df_aapl['estimated_bid_ask_spread'].astype('float64')\n",
        "df_aapl['estimated_obd'] = df_aapl['estimated_obd'].astype('float64')\n",
        "df_aapl['50_day_sma'] = df_aapl['50_day_sma'].astype('float64')\n",
        "df_aapl['news_impact'] = df_aapl['news_impact'].astype('int8')\n",
        "\n",
        "# Sorting by timestamp for inspection\n",
        "df_aapl = df_aapl.sort_values('timestamp')\n",
        "\n",
        "# Saving\n",
        "output_csv = \"aapl_data.csv\"\n",
        "df_aapl.to_csv(output_csv, index=False, date_format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(f\"Rows: {len(df_aapl)}\")\n",
        "print({df_aapl.columns.tolist()})\n",
        "print(df_aapl.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec39f47",
      "metadata": {},
      "source": [
        "#### Cleaning directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bf1d45",
      "metadata": {},
      "outputs": [],
      "source": [
        "directory_to_delete = '/content/backtesting_final'\n",
        "try:\n",
        "    shutil.rmtree(directory_to_delete)\n",
        "    print(f\"Directory '{directory_to_delete}' and its contents successfully deleted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Directory '{directory_to_delete}' not found.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error deleting directory '{directory_to_delete}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TOtFxl0iJh7C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOtFxl0iJh7C",
        "outputId": "281bfc0f-ffef-457c-e81e-1dbf8ec9a576"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:distributed.scheduler:Remove client Client-8ee56be6-2458-11f0-96c1-0242c0a80a0a\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38072; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-8ee56be6-2458-11f0-96c1-0242c0a80a0a\n",
            "INFO:distributed.scheduler:Close client connection: Client-8ee56be6-2458-11f0-96c1-0242c0a80a0a\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1745865296.6417468') (0, 1, 2, 3, 4, 5, 6, 7)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:33273'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:39025'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37055'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:46477'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35665'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:38403'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36677'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:42157'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38060; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38028; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38044; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:37992; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38020; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38008; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38038; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:45973 name: 0 (stimulus_id='handle-worker-cleanup-1745865296.6975975')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:34971 name: 1 (stimulus_id='handle-worker-cleanup-1745865296.6997302')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:40013 name: 2 (stimulus_id='handle-worker-cleanup-1745865296.7017734')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:38191 name: 3 (stimulus_id='handle-worker-cleanup-1745865296.7047606')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:34481 name: 4 (stimulus_id='handle-worker-cleanup-1745865296.7082255')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:38031 name: 5 (stimulus_id='handle-worker-cleanup-1745865296.7104268')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:41623 name: 6 (stimulus_id='handle-worker-cleanup-1745865296.7128115')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:38068; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:45885 name: 7 (stimulus_id='handle-worker-cleanup-1745865296.7199528')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:39025' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:46477' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:37055' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:35665' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:42157' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:33273' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:36677' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:38403' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        }
      ],
      "source": [
        "# Closing the client and cluster\n",
        "client.close()\n",
        "cluster.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "backtesting_subset_nasdaq100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
