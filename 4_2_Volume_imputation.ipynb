{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546437f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from neuralforecast import NeuralForecast\n",
    "import logging\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import glob\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e6a3e",
   "metadata": {},
   "source": [
    "# Predicting volume \n",
    "- For the 3 months of optimization and simulation periods (collectively 6 months) all tickers with the TFT.\n",
    "- Needs to be more optimized.\n",
    "- Unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory and loading tickers\n",
    "base_dir = \"/home/jupyter-kohv04@vse.cz/kohv04/backtesting_final/\"\n",
    "metadata_dir = os.path.join(base_dir, \"metadata\")\n",
    "nasdaq100_tickers = pd.read_json(os.path.join(metadata_dir, \"nasdaq100_ticker_dataset.json\"))[\"Ticker\"].tolist()[17:]\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)s] - [%(process)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('prediction_process.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Time parameters\n",
    "trading_start = pd.to_datetime(\"09:30:00\").time()\n",
    "trading_end = pd.to_datetime(\"16:00:00\").time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_volume_prediction_dir(ticker):\n",
    "    \"\"\"Create volume_prediction directory for a given ticker.\"\"\"\n",
    "    volume_pred_dir = os.path.join(base_dir, f\"ticker={ticker}_standardized/volume_prediction\")\n",
    "    os.makedirs(volume_pred_dir, exist_ok=True)\n",
    "    return volume_pred_dir\n",
    "\n",
    "def preprocess_ticker_data(ticker, period, n_lags=15):\n",
    "    \"\"\"Load and prepare data for a specific ticker and period.\"\"\"\n",
    "    period_dir = os.path.join(base_dir, f\"ticker={ticker}_standardized/{period}\")\n",
    "    df = pd.concat([pd.read_parquet(f) for f in glob.glob(os.path.join(period_dir, \"part.*.parquet\"))])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "    \n",
    "    # Regime data\n",
    "    ticker_regime = pd.read_csv(os.path.join(base_dir, f\"ticker={ticker}/{ticker}_regimes.csv\"))\n",
    "    ticker_regime[\"date\"] = pd.to_datetime(ticker_regime[\"date\"]).dt.date\n",
    "    \n",
    "    # Loading scalers and encoders\n",
    "    volume_pred_dir = create_volume_prediction_dir(ticker)\n",
    "    scaler = pd.read_pickle(os.path.join(volume_pred_dir, \"scaler.pkl\"))\n",
    "    log_volume_scaler = pd.read_pickle(os.path.join(volume_pred_dir, \"log_volume_scaler.pkl\"))\n",
    "    regime_encoders = {\n",
    "        col: pd.read_pickle(os.path.join(volume_pred_dir, f\"{col}_encoder.pkl\"))\n",
    "        for col in [\"volatility_regime\", \"trend_regime\", \"liquidity_regime\"]\n",
    "    }\n",
    "    \n",
    "    return df, ticker_regime, scaler, log_volume_scaler, regime_encoders\n",
    "\n",
    "def preprocess_row(window, ticker_regime, scaler, log_volume_scaler, regime_encoders, n_lags=15):\n",
    "    \"\"\"Preprocess a window of data for prediction.\"\"\"\n",
    "    window = window.copy()\n",
    "    \n",
    "    # Temporal features\n",
    "    window[\"hour\"] = window[\"timestamp\"].dt.hour\n",
    "    window[\"day_of_week\"] = window[\"timestamp\"].dt.dayofweek\n",
    "    window[\"minute\"] = window[\"timestamp\"].dt.minute\n",
    "    window[\"time_since_open\"] = ((window[\"timestamp\"] - window[\"timestamp\"].dt.floor(\"D\") - \n",
    "                                 pd.Timedelta(hours=9, minutes=30)).dt.total_seconds() / 60).clip(lower=0)\n",
    "    window[\"is_trading\"] = ((window[\"timestamp\"].dt.time >= trading_start) & \n",
    "                            (window[\"timestamp\"].dt.time <= trading_end)).astype(int)\n",
    "    window[\"date\"] = window[\"timestamp\"].dt.date\n",
    "    window[\"intraday_minute\"] = (((window[\"timestamp\"] - window[\"timestamp\"].dt.floor(\"D\") - \n",
    "                                  pd.Timedelta(hours=9, minutes=30)).dt.total_seconds() / 60).astype(int) + 1)\n",
    "    \n",
    "    # Merging with regime data.\n",
    "    window = window.merge(ticker_regime[[\"date\", \"volatility_regime\", \"trend_regime\", \"liquidity_regime\", \"news_impact\"]], \n",
    "                          on=\"date\", how=\"left\").rename(columns={\"news_impact\": \"news_impact_regime\"})\n",
    "    \n",
    "    # Lagged features\n",
    "    for i in range(1, n_lags + 1):\n",
    "        window[f\"volume_lag_{i}\"] = window[\"volume\"].shift(i)\n",
    "        window[f\"close_lag_{i}\"] = window[\"close\"].shift(i)\n",
    "    \n",
    "    # Scaling continuous features\n",
    "    cont_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"estimated_obd\", \"estimated_bid_ask_spread\", \n",
    "                 \"prev_session_high\", \"prev_session_low\", \"50_day_sma\"] + \\\n",
    "                [f\"volume_lag_{i}\" for i in range(1, n_lags + 1)] + [f\"close_lag_{i}\" for i in range(1, n_lags + 1)]\n",
    "    window[cont_cols] = scaler.transform(window[cont_cols])\n",
    "    \n",
    "    window[\"log_volume\"] = np.log1p(window[\"volume\"])\n",
    "    window[\"log_volume\"] = log_volume_scaler.transform(window[[\"log_volume\"]]) * 2 - 1\n",
    "    \n",
    "    # Encoding regime data\n",
    "    for col in [\"volatility_regime\", \"trend_regime\", \"liquidity_regime\"]:\n",
    "        window[col] = regime_encoders[col].transform(window[col].astype(str))\n",
    "    \n",
    "    # Calculating returns\n",
    "    window[\"returns\"] = window[\"close\"].pct_change().fillna(0)\n",
    "    \n",
    "    # Imputing NaNs for lag features\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        window[f\"volume_lag_{lag}\"] = window[f\"volume_lag_{lag}\"].fillna(\n",
    "            window[\"volume\"].iloc[0] if window[\"timestamp\"].dt.time.iloc[0] < trading_start else window[\"volume\"])\n",
    "        window[f\"close_lag_{lag}\"] = window[f\"close_lag_{lag}\"].fillna(\n",
    "            window[\"close\"].iloc[0] if window[\"timestamp\"].dt.time.iloc[0] < trading_start else window[\"close\"])\n",
    "    \n",
    "    # Preparing NeuralForecast data for the current row\n",
    "    current_row = window.iloc[-1:].copy()\n",
    "    nf_data = current_row[[\"ticker\", \"timestamp\", \"log_volume\"]].rename(\n",
    "        columns={\"ticker\": \"unique_id\", \"timestamp\": \"ds\", \"log_volume\": \"y\"})\n",
    "    exogenous_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"estimated_obd\", \"estimated_bid_ask_spread\", \n",
    "                      \"prev_session_high\", \"prev_session_low\", \"50_day_sma\", \"hour\", \"day_of_week\", \"minute\", \n",
    "                      \"time_since_open\", \"is_trading\", \"volatility_regime\", \"trend_regime\", \"liquidity_regime\", \n",
    "                      \"news_impact_regime\", \"returns\", \"intraday_minute\"] + \\\n",
    "                     [f\"volume_lag_{i}\" for i in range(1, n_lags + 1)] + [f\"close_lag_{i}\" for i in range(1, n_lags + 1)]\n",
    "    nf_data = nf_data.join(current_row[exogenous_cols])\n",
    "    return nf_data\n",
    "\n",
    "def process_ticker_period(ticker, period, n_lags=15, input_size=60):\n",
    "    \"\"\"Process a single ticker and period (optimization or simulation).\"\"\"\n",
    "    logger.info(f\"Starting processing for {ticker} in {period}\")\n",
    "    volume_pred_dir = create_volume_prediction_dir(ticker)\n",
    "    \n",
    "    # Loading the trained TFT model\n",
    "    try:\n",
    "        nf = NeuralForecast.load(path=os.path.join(volume_pred_dir, f\"models/{ticker}_model\"))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model for {ticker}: {e}\")\n",
    "        return\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, f\"ticker={ticker}_standardized/{period}_tft\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loading data and preprocessing artifacts\n",
    "    df, ticker_regime, scaler, log_volume_scaler, regime_encoders = preprocess_ticker_data(ticker, period, n_lags=n_lags)\n",
    "    \n",
    "    # Generating predictions using a rolling window\n",
    "    predictions_list = []\n",
    "    for idx in range(input_size - 1, len(df) - 15):\n",
    "        # Preparing window up to the current timestamp\n",
    "        window = df.iloc[max(0, idx - input_size + 1):idx + 1].copy()\n",
    "        if len(window) < input_size:\n",
    "            continue\n",
    "        \n",
    "        # Preprocessing the window\n",
    "        nf_data = preprocess_row(window, ticker_regime, scaler, log_volume_scaler, regime_encoders, n_lags=n_lags)\n",
    "        \n",
    "        # if not in trading hours -> Skip\n",
    "        if nf_data[\"is_trading\"].iloc[0] != 1:\n",
    "            continue\n",
    "        \n",
    "        # Preparing future data\n",
    "        futr_timestamps = pd.date_range(start=df[\"timestamp\"].iloc[idx] + pd.Timedelta(minutes=1), \n",
    "                                        periods=15, freq=\"1min\")\n",
    "        futr_data = pd.DataFrame({\"ds\": futr_timestamps, \"unique_id\": ticker})\n",
    "        futr_data[\"hour\"] = futr_data[\"ds\"].dt.hour\n",
    "        futr_data[\"day_of_week\"] = futr_data[\"ds\"].dt.dayofweek\n",
    "        futr_data[\"minute\"] = futr_data[\"ds\"].dt.minute\n",
    "        futr_data[\"time_since_open\"] = ((futr_data[\"ds\"] - futr_data[\"ds\"].dt.floor(\"D\") - \n",
    "                                        pd.Timedelta(hours=9, minutes=30)).dt.total_seconds() / 60).clip(lower=0)\n",
    "        futr_data[\"is_trading\"] = ((futr_data[\"ds\"].dt.time >= trading_start) & \n",
    "                                   (futr_data[\"ds\"].dt.time <= trading_end)).astype(int)\n",
    "        futr_data[\"date\"] = futr_data[\"ds\"].dt.date\n",
    "        futr_data = futr_data.merge(ticker_regime[[\"date\", \"volatility_regime\", \"trend_regime\", \n",
    "                                                   \"liquidity_regime\", \"news_impact\"]], \n",
    "                                    on=\"date\", how=\"left\").rename(columns={\"news_impact\": \"news_impact_regime\"})\n",
    "        \n",
    "        # Encoding future regime data\n",
    "        for col in [\"volatility_regime\", \"trend_regime\", \"liquidity_regime\"]:\n",
    "            futr_data[col] = regime_encoders[col].transform(futr_data[col].astype(str))\n",
    "        \n",
    "        # PREDICTIONS\n",
    "        try:\n",
    "            pred = nf.predict(df=nf_data, futr_df=futr_data)\n",
    "            pred_at_horizon_15 = pred[pred[\"ds\"] == futr_timestamps[-1]][\"TFT\"].values[0]\n",
    "            predictions_list.append({\"timestamp\": df[\"timestamp\"].iloc[idx], \n",
    "                                     \"pred_volume_15_tft_scaled\": pred_at_horizon_15})\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Prediction failed at index {idx} for {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not predictions_list:\n",
    "        logger.warning(f\"No predictions generated for {ticker} in {period}\")\n",
    "        return\n",
    "    \n",
    "    # Processing predictions\n",
    "    predictions_df = pd.DataFrame(predictions_list)\n",
    "    predictions_df[\"pred_volume_15_tft\"] = np.expm1(log_volume_scaler.inverse_transform(\n",
    "        ((predictions_df[\"pred_volume_15_tft_scaled\"] + 1) / 2).values.reshape(-1, 1)).flatten())\n",
    "    \n",
    "    # Merging with original dataset\n",
    "    df = df.merge(predictions_df[[\"timestamp\", \"pred_volume_15_tft\"]], on=\"timestamp\", how=\"left\")\n",
    "    \n",
    "    # Saving to new subfolder\n",
    "    df.to_parquet(os.path.join(output_dir, \"data.parquet\"), index=False)\n",
    "    logger.info(f\"Saved updated {period} data for {ticker} to {output_dir}\")\n",
    "\n",
    "async def run_ticker_period(ticker, period, executor, n_lags=15, input_size=60):\n",
    "    \"\"\"Run the processing for a ticker and period asynchronously.\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    func = partial(process_ticker_period, ticker, period, n_lags, input_size)\n",
    "    await loop.run_in_executor(executor, func)\n",
    "\n",
    "async def process_all_tickers():\n",
    "    \"\"\"Process all tickers and periods asynchronously with parallelization.\"\"\"\n",
    "    max_workers = min(len(nasdaq100_tickers) * 2, os.cpu_count() * 2)  # Adjust based on CPU cores\n",
    "    logger.info(f\"Using {max_workers} workers for parallel processing\")\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = []\n",
    "        for ticker in nasdaq100_tickers:\n",
    "            for period in [\"optimization\", \"simulation\"]:\n",
    "                tasks.append(run_ticker_period(ticker, period, executor))\n",
    "        \n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Running the asynchronous processing\n",
    "    asyncio.run(process_all_tickers())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
