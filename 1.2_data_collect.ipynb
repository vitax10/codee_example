{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASDAQ 100 data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "api_key = '#################################'\n",
    "\n",
    "# nasdaq100_ticker_dataset.json tickers\n",
    "with open(\"nasdaq100_ticker_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    tickers = [item[\"Ticker\"] for item in data]\n",
    "\n",
    "print(f\"{len(tickers)} tickers loaded from nasdaq100_ticker_dataset.json\")\n",
    "\n",
    "date_from = '2020-01-01'\n",
    "date_to = '2025-02-17'\n",
    "\n",
    "# API request handler with pagination, rate limiting, and detailed logging\n",
    "def fetch_data(url, feature_name, ticker=None, max_retries=3):\n",
    "    \"\"\"Handles API requests with pagination, rate limiting, and detailed logging\"\"\"\n",
    "    all_results = []\n",
    "    retry_count = 0\n",
    "    \n",
    "    while url and retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # Log response details\n",
    "                print(f\"API Response for {feature_name} ({ticker or 'GENERAL'}):\")\n",
    "                print(f\"Status: {data.get('status')}\")\n",
    "                print(f\"Query Count: {data.get('queryCount')}\")\n",
    "                print(f\"Results Count: {data.get('resultsCount')}\")\n",
    "                print(f\"Next URL: {data.get('next_url')}\")\n",
    "                \n",
    "                if 'results' in data:\n",
    "                    all_results.extend(data['results'])\n",
    "                \n",
    "                # Handling pagination\n",
    "                url = data.get('next_url', None)\n",
    "                if url:\n",
    "                    print(f\"Fetching next page for {feature_name} on {ticker or 'GENERAL'}...\")\n",
    "                    url = f\"{url}&apiKey={api_key}\"  # Appending API key to next_url\n",
    "                    retry_count = 0  # Resetting retry count for new page\n",
    "            elif response.status_code == 429:  # Too many requests\n",
    "                print(f\"API rate limit exceeded. Waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                retry_count += 1\n",
    "            else:\n",
    "                print(f\"Error {response.status_code} for {feature_name} on {ticker or 'GENERAL'}: {response.text}\")\n",
    "                retry_count += 1\n",
    "                time.sleep(5)  # Small delay before retrying\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request Error: {e}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(5)  # Small delay before retrying\n",
    "    \n",
    "    if retry_count >= max_retries:\n",
    "        print(f\"Max retries reached for {feature_name} on {ticker or 'GENERAL'}\")\n",
    "    \n",
    "    return {'results': all_results} if all_results else {}\n",
    "\n",
    "# Step 1: Fetching OHLCV Data & Save\n",
    "ohlcv_data = []\n",
    "missing_tickers = []  \n",
    "available_start_date = set()\n",
    "available_end_date = set()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching OHLCV data for {ticker}...\")\n",
    "    url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{date_from}/{date_to}?apiKey={api_key}'\n",
    "    stock_data = fetch_data(url, \"Stock Data\", ticker)\n",
    "    \n",
    "    if not stock_data or 'results' not in stock_data or not stock_data['results']:\n",
    "        print(f\"No OHLCV data for {ticker}, skipping...\")\n",
    "        missing_tickers.append(ticker)\n",
    "        continue  \n",
    "\n",
    "    dates_available = [pd.to_datetime(item['t'], unit='ms').date() for item in stock_data['results']]\n",
    "    min_date = min(dates_available)\n",
    "    max_date = max(dates_available)\n",
    "    print(f\"Data for {ticker} spans {min_date} to {max_date}\")\n",
    "    if min_date > pd.to_datetime(date_from).date() or max_date < pd.to_datetime(date_to).date():\n",
    "        print(f\"Warning: Incomplete date range for {ticker}. Expected {date_from} to {date_to}\")\n",
    "\n",
    "    # Identifying available dates\n",
    "    dates_available_set = set(dates_available)\n",
    "    if pd.to_datetime(date_from).date() in dates_available_set:\n",
    "        available_start_date.add(ticker)\n",
    "    if pd.to_datetime(date_to).date() in dates_available_set:\n",
    "        available_end_date.add(ticker)\n",
    "\n",
    "    # Storing OHLCV data\n",
    "    for minute_data in stock_data['results']:\n",
    "        row = {\n",
    "            \"Date\": pd.to_datetime(minute_data['t'], unit='ms').date(),\n",
    "            \"Time\": pd.to_datetime(minute_data['t'], unit='ms').time(),\n",
    "            \"Ticker\": ticker,\n",
    "            \"Open\": minute_data['o'],\n",
    "            \"High\": minute_data['h'],\n",
    "            \"Low\": minute_data['l'],\n",
    "            \"Close\": minute_data['c'],\n",
    "            \"Volume\": minute_data['v']\n",
    "        }\n",
    "        ohlcv_data.append(row)\n",
    "\n",
    "# Converting and saving\n",
    "df = pd.DataFrame(ohlcv_data)\n",
    "df.to_parquet(\"backtest_data_step1.parquet\", index=False)\n",
    "\n",
    "with open(\"missing_tickers.json\", \"w\") as f:\n",
    "    json.dump(missing_tickers, f, indent=4)\n",
    "\n",
    "print(f\"Total tickers processed: {len(tickers)}\")\n",
    "print(f\"Total tickers missing data: {len(missing_tickers)}\")\n",
    "print(f\"Tickers with data from start date: {len(available_start_date)}\")\n",
    "print(f\"Tickers with data to end date: {len(available_end_date)}\")\n",
    "\n",
    "# Step 2: Computing Previous Session High/Low & Saving\n",
    "df = pd.read_parquet(\"backtest_data_step1.parquet\")\n",
    "df.sort_values(by=[\"Ticker\", \"Date\", \"Time\"], inplace=True)\n",
    "\n",
    "daily_high_low = df.groupby([\"Ticker\", \"Date\"]).agg(\n",
    "    Prev_Session_High=(\"High\", \"max\"),\n",
    "    Prev_Session_Low=(\"Low\", \"min\")\n",
    ").reset_index()\n",
    "\n",
    "daily_high_low[\"Prev_Trading_Date\"] = daily_high_low.groupby(\"Ticker\")[\"Date\"].shift(1)\n",
    "daily_high_low[\"Prev_Trading_Date\"] = daily_high_low.groupby(\"Ticker\")[\"Prev_Trading_Date\"].ffill()\n",
    "\n",
    "df = df.merge(\n",
    "    daily_high_low[[\"Ticker\", \"Date\", \"Prev_Trading_Date\"]],\n",
    "    on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    daily_high_low[[\"Ticker\", \"Date\", \"Prev_Session_High\", \"Prev_Session_Low\"]],\n",
    "    left_on=[\"Ticker\", \"Prev_Trading_Date\"],\n",
    "    right_on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_Prev\")\n",
    ")\n",
    "\n",
    "df.drop(columns=[\"Prev_Trading_Date\", \"Date_Prev\"], inplace=True)\n",
    "first_day = df[\"Date\"].min()\n",
    "df = df[df[\"Date\"] != first_day]\n",
    "\n",
    "df.to_parquet(\"backtest_data_step2.parquet\", index=False)\n",
    "\n",
    "# Step 3: Computing Bid-Ask Spread & OBD & Saving\n",
    "def compute_bid_ask_spread(df):\n",
    "    df[\"Bid-Ask Spread (Estimated)\"] = df[\"High\"] - df[\"Low\"]\n",
    "    df[\"Bid-Ask Spread (Estimated)\"] = df[\"Bid-Ask Spread (Estimated)\"].apply(lambda x: max(x, 0.0001))\n",
    "    return df\n",
    "\n",
    "def compute_obd(df):\n",
    "    if \"Bid-Ask Spread (Estimated)\" not in df.columns:\n",
    "        df = compute_bid_ask_spread(df)\n",
    "    df[\"Estimated OBD\"] = df[\"Volume\"] / df[\"Bid-Ask Spread (Estimated)\"]\n",
    "    return df\n",
    "\n",
    "df = pd.read_parquet(\"backtest_data_step2.parquet\")\n",
    "df = compute_bid_ask_spread(df)\n",
    "df = compute_obd(df)\n",
    "df.to_parquet(\"backtest_data_step3.parquet\", index=False)\n",
    "\n",
    "# Step 4: Computing 50-day SMA using daily close prices and saving\n",
    "df = pd.read_parquet(\"backtest_data_step3.parquet\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "daily_closes = df.groupby([\"Ticker\", \"Date\"])[\"Close\"].last().reset_index()\n",
    "daily_closes[\"50-day SMA\"] = daily_closes.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=50, min_periods=1).mean())\n",
    "\n",
    "df = df.merge(daily_closes[[\"Ticker\", \"Date\", \"50-day SMA\"]], on=[\"Ticker\", \"Date\"], how=\"left\")\n",
    "df = df[df[\"Date\"] >= \"2020-02-17\"]\n",
    "\n",
    "df.to_parquet(\"backtest_data_step4.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest data - 5 years back\n",
    "https://github.com/datasets/nasdaq-listings/tree/main   -> Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "api_key = '####################################'\n",
    "\n",
    "# Loading tickers from json\n",
    "with open(\"nasdaq_tickers_17_02_2025.json\", \"r\") as file:\n",
    "    tickers = json.load(file) # Limit to first ticker for testing [11:12]  for AAPL\n",
    "\n",
    "date_from = '2020-01-01'\n",
    "date_to = '2025-02-17'\n",
    "\n",
    "# API request handler with rate limiting\n",
    "def fetch_data(url, feature_name, ticker=None, max_retries=3):\n",
    "    \"\"\"Handles API requests with rate limiting and error logging\"\"\"\n",
    "    all_results = []\n",
    "    retry_count = 0\n",
    "    \n",
    "    while url and retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'results' in data:\n",
    "                    all_results.extend(data['results'])\n",
    "                \n",
    "                # Handling pagination\n",
    "                url = data.get('next_url', None)\n",
    "                if url:\n",
    "                    print(f\"Fetching next page for {feature_name} on {ticker or 'GENERAL'}...\")\n",
    "                    url = f\"{url}&apiKey={api_key}\"  # Appending API key to next_url\n",
    "                    retry_count = 0  # Resetting retry count for new page\n",
    "            elif response.status_code == 429:  # Too many requests\n",
    "                print(f\"API rate limit exceeded. Waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                retry_count += 1\n",
    "            else:\n",
    "                print(f\"Error {response.status_code} for {feature_name} on {ticker or 'GENERAL'}: {response.text}\")\n",
    "                retry_count += 1\n",
    "                time.sleep(5)  # Small delay before retrying\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request Error: {e}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(5)  # Small delay before retrying\n",
    "    \n",
    "    if retry_count >= max_retries:\n",
    "        print(f\"Max retries reached for {feature_name} on {ticker or 'GENERAL'}\")\n",
    "    \n",
    "    return {'results': all_results} if all_results else {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fetching OHLCV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_data = []\n",
    "missing_tickers = []  # Tracking missing tickers\n",
    "available_start_date = set()\n",
    "available_end_date = set()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching OHLCV data for {ticker}...\")\n",
    "    url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{date_from}/{date_to}?apiKey={api_key}'\n",
    "    stock_data = fetch_data(url, \"Stock Data\", ticker)\n",
    "    \n",
    "    if not stock_data or 'results' not in stock_data:\n",
    "        print(f\"No OHLCV data for {ticker}, skipping...\")\n",
    "        missing_tickers.append(ticker)\n",
    "        continue  \n",
    "\n",
    "    # Identifying available dates\n",
    "    dates_available = set(pd.to_datetime(item['t'], unit='ms').date() for item in stock_data['results'])\n",
    "    \n",
    "    if pd.to_datetime(date_from).date() in dates_available:\n",
    "        available_start_date.add(ticker)\n",
    "    if pd.to_datetime(date_to).date() in dates_available:\n",
    "        available_end_date.add(ticker)\n",
    "\n",
    "    # Storing OHLCV data\n",
    "    for minute_data in stock_data['results']:\n",
    "        row = {\n",
    "            \"Date\": pd.to_datetime(minute_data['t'], unit='ms').date(),\n",
    "            \"Time\": pd.to_datetime(minute_data['t'], unit='ms').time(),\n",
    "            \"Ticker\": ticker,\n",
    "            \"Open\": minute_data['o'],\n",
    "            \"High\": minute_data['h'],\n",
    "            \"Low\": minute_data['l'],\n",
    "            \"Close\": minute_data['c'],\n",
    "            \"Volume\": minute_data['v']\n",
    "        }\n",
    "        ohlcv_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(ohlcv_data)\n",
    "df.to_parquet(\"backtest_data_step1.parquet\", index=False)\n",
    "\n",
    "# Saving missing tickers\n",
    "with open(\"missing_tickers.json\", \"w\") as f:\n",
    "    json.dump(missing_tickers, f, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Total tickers processed: {len(tickers)}\")\n",
    "print(f\"Total tickers missing data: {len(missing_tickers)}\")\n",
    "print(f\"Tickers with data from start date: {len(available_start_date)}\")\n",
    "print(f\"Tickers with data to end date: {len(available_end_date)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Computing previous session High/Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHLCV data from Step 1\n",
    "df = pd.read_parquet(\"backtest_data_step1.parquet\")\n",
    "\n",
    "# Sorting data\n",
    "df.sort_values(by=[\"Ticker\", \"Date\", \"Time\"], inplace=True)\n",
    "\n",
    "# Computing the daily high and low per ticker\n",
    "daily_high_low = df.groupby([\"Ticker\", \"Date\"]).agg(\n",
    "    Prev_Session_High=(\"High\", \"max\"),\n",
    "    Prev_Session_Low=(\"Low\", \"min\")\n",
    ").reset_index()\n",
    "\n",
    "# Ensuring previous session values align with actual trading days\n",
    "daily_high_low[\"Prev_Trading_Date\"] = daily_high_low.groupby(\"Ticker\")[\"Date\"].shift(1)\n",
    "\n",
    "# Forward-filling missing previous session dates (handles non-continuous trading)\n",
    "daily_high_low[\"Prev_Trading_Date\"] = daily_high_low.groupby(\"Ticker\")[\"Prev_Trading_Date\"].ffill()\n",
    "\n",
    "# Merging previous session high/low back to the main dataset\n",
    "df = df.merge(\n",
    "    daily_high_low[[\"Ticker\", \"Date\", \"Prev_Trading_Date\"]],\n",
    "    on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Merging again to get the correct high/low from the actual previous session\n",
    "df = df.merge(\n",
    "    daily_high_low[[\"Ticker\", \"Date\", \"Prev_Session_High\", \"Prev_Session_Low\"]],\n",
    "    left_on=[\"Ticker\", \"Prev_Trading_Date\"],\n",
    "    right_on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_Prev\")\n",
    ")\n",
    "\n",
    "# Dropping extra columns\n",
    "df.drop(columns=[\"Prev_Trading_Date\", \"Date_Prev\"], inplace=True)\n",
    "\n",
    "# Dropping the first available date since it has NaN values for previous session data\n",
    "first_day = df[\"Date\"].min()\n",
    "df = df[df[\"Date\"] != first_day]\n",
    "\n",
    "# Saving\n",
    "df.to_parquet(\"backtest_data_step2.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Estimating Bid-Ask spread and OBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bid_ask_spread(df):\n",
    "    df[\"Bid-Ask Spread (Estimated)\"] = df[\"High\"] - df[\"Low\"]\n",
    "    df[\"Bid-Ask Spread (Estimated)\"] = df[\"Bid-Ask Spread (Estimated)\"].apply(lambda x: max(x, 0.0001))  # Use a more realistic min value\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_obd(df):\n",
    "    if \"Bid-Ask Spread (Estimated)\" not in df.columns:\n",
    "        df = compute_bid_ask_spread(df)\n",
    "    df[\"Estimated OBD\"] = df[\"Volume\"] / df[\"Bid-Ask Spread (Estimated)\"]\n",
    "    return df\n",
    "\n",
    "df = pd.read_parquet(\"backtest_data_step2.parquet\")\n",
    "df = compute_bid_ask_spread(df)\n",
    "df = compute_obd(df)\n",
    "df.to_parquet(\"backtest_data_step3.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Computing 50-day SMA using daily close prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"backtest_data_step3.parquet\")\n",
    "\n",
    "# Converting Date column to datetime\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Creating a DataFrame with only daily closing prices\n",
    "daily_closes = df.groupby([\"Ticker\", \"Date\"])[\"Close\"].last().reset_index()\n",
    "\n",
    "# Computing 50-day SMA using rolling window on daily data\n",
    "daily_closes[\"50-day SMA\"] = daily_closes.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=50, min_periods=1).mean())\n",
    "\n",
    "# Merging the SMA values back into the original intraday DataFrame\n",
    "df = df.merge(daily_closes[[\"Ticker\", \"Date\", \"50-day SMA\"]], on=[\"Ticker\", \"Date\"], how=\"left\")\n",
    "\n",
    "# Dropping extra early dates\n",
    "df = df[df[\"Date\"] >= \"2020-02-17\"]\n",
    "\n",
    "\n",
    "df.to_parquet(\"backtest_data_step4.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
