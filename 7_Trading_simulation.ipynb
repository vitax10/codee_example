{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4128825b-7d87-40fe-a8d0-b4f00543945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install quantstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239df03-84ea-4c12-a5df-9f28321ac21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import traceback\n",
    "from typing import Optional, Dict\n",
    "import quantstats as qs \n",
    "\n",
    "# --- Environment Setup ---\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{os.environ.get('HOME')}/kohv04/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "sys.path.append('/home/jupyter-kohv04@vse.cz/kohv04/lib/python3.10/site-packages')\n",
    "\n",
    "try:\n",
    "    import talib\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import TA-Lib: {e}\")\n",
    "import vectorbt as vbt\n",
    "from numba import njit\n",
    "from vectorbt.portfolio.enums import SizeType\n",
    "\n",
    "# -global settings\n",
    "vbt.settings.caching['enabled'] = False\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning, module=\"pandas.core.frame\")\n",
    "\n",
    "# config and constrants\n",
    "BASE_DIR = \"/home/jupyter-kohv04@vse.cz/kohv04/backtesting_final/\"\n",
    "METADATA_FILE = f\"{BASE_DIR}/metadata/nasdaq100_ticker_dataset.json\"\n",
    "INITIAL_CASH = 1_000_000\n",
    "REJECT_PROBABILITY = 0.005 # 0.5% chance of order rejection\n",
    "\n",
    "# Best Parameter Grids for Simulation\n",
    "BEST_BASELINE_BREAKOUT_PARAMS = {'sl_stop': [0.011], 'tp_stop': [0.01]}\n",
    "BEST_BASELINE_BBANDS_PARAMS = {'timeperiod': [25], 'nbdev': [2.5]}\n",
    "BEST_BASELINE_MOMENTUM_PARAMS = {'window': [6], 'sl_stop': [0.024]}\n",
    "BEST_VOLUME_MOMENTUM_PARAMS = {'timeperiod': [17], 'kappa_vol_mom': [2.8], 'adx_threshold': [33], 'alpha_atr': [3.3]}\n",
    "BEST_VOLUME_BREAKOUT_PARAMS = {'phi_va': [0.80], 'kappa_surge': [3.5], 'timeperiod': [20], 'adx_threshold': [20], 'alpha_atr': [3.0], 'alpha_tp': [6.0]}\n",
    "BEST_VOLUME_VWAP_REVERSION_PARAMS = {'window': [40], 'quantile': [0.9], 'slope': [0.0001], 'tau_vwap_trend': [19], 'alpha_atr': [3.2], 'alpha_tp': [6.0]}\n",
    "BEST_DL_BREAKOUT_PARAMS = {'phi_va': [0.75], 'kappa_dl': [1.6], 'timeperiod': [16], 'adx_threshold': [20], 'alpha_atr': [3.0], 'alpha_tp': [4.0]}\n",
    "BEST_DL_VOLUME_MOMENTUM_PARAMS = {'timeperiod': [20], 'kappa_dl': [1.5], 'adx_threshold': [30], 'alpha_atr': [4.0], 'tau_vol_trend': [7]}\n",
    "BEST_DL_VWAP_REVERSION_PARAMS = {'delta_vwap': [0.003], 'tau_vwap_trend': [18], 'volume_multiplier': [1.5], 'alpha_atr': [3.0], 'alpha_tp': [7.0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4c95c-c60b-40dd-9f0c-a216eff6da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simulation_data(ticker: str, strategy_type: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Loads and combines parquet files for a given ticker and strategy type for simulation.\"\"\"\n",
    "    path = os.path.join(BASE_DIR, f\"ticker={ticker}_standardized\", f\"simulation_indicators_{strategy_type}\")\n",
    "    all_files = glob.glob(os.path.join(path, \"part.*.parquet\"))\n",
    "    if not all_files:\n",
    "        print(f\"Warning: No simulation files found for {ticker} in {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.concat((pd.read_parquet(f) for f in all_files), ignore_index=True)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading simulation data for {ticker} - {strategy_type}: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_baseline_breakout(df: pd.DataFrame, sl_stop: list, tp_stop: list, **kwargs):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    param_index = pd.MultiIndex.from_arrays([sl_stop, tp_stop], names=['sl_stop', 'tp_stop'])\n",
    "    broadcast_target = pd.DataFrame(index=df.index, columns=param_index)\n",
    "    price_df = df['close'].vbt.broadcast_to(broadcast_target)\n",
    "    time_mask = (df.index.hour < 15)\n",
    "    time_filter = pd.Series(time_mask, index=df.index).vbt.broadcast_to(price_df)\n",
    "    long_entries = df['breakout_buy'].vbt.broadcast_to(broadcast_target).fillna(False) & time_filter\n",
    "    short_entries = df['breakout_sell'].vbt.broadcast_to(broadcast_target).fillna(False) & time_filter\n",
    "    sl_stop_params = broadcast_target.columns.get_level_values('sl_stop').to_numpy()\n",
    "    tp_stop_params = broadcast_target.columns.get_level_values('tp_stop').to_numpy()\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        price_df, entries=long_entries, short_entries=short_entries, sl_stop=sl_stop_params,\n",
    "        tp_stop=tp_stop_params, freq='1min', init_cash=INITIAL_CASH, fees=0.0001,\n",
    "        slippage=0.0002, size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_baseline_bbands(df: pd.DataFrame, timeperiod: list, nbdev: list, **kwargs):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    bbands = vbt.talib('BBANDS').run(df['close'], timeperiod=timeperiod[0], nbdevup=nbdev[0], nbdevdn=nbdev[0])\n",
    "    price_df = df['close'].vbt.broadcast_to(bbands.lowerband)\n",
    "    time_mask = (df.index.hour < 15)\n",
    "    time_filter = pd.Series(time_mask, index=df.index).vbt.broadcast_to(price_df)\n",
    "    long_entries = (price_df < bbands.lowerband) & time_filter\n",
    "    long_exits = price_df > bbands.middleband\n",
    "    short_entries = (price_df > bbands.upperband) & time_filter\n",
    "    short_exits = price_df < bbands.middleband\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        price_df, entries=long_entries, exits=long_exits, short_entries=short_entries,\n",
    "        short_exits=short_exits, freq='1min', init_cash=INITIAL_CASH, fees=0.0001,\n",
    "        slippage=0.0002, size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_baseline_momentum(df: pd.DataFrame, window: list, sl_stop: list, **kwargs):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    is_up = (df['close'] > df['open']).astype(int)\n",
    "    is_down = (df['close'] < df['open']).astype(int)\n",
    "    consecutive_up = is_up.rolling(window=window[0]).sum()\n",
    "    long_entries = (consecutive_up >= window[0])\n",
    "    consecutive_down = is_down.rolling(window=window[0]).sum()\n",
    "    short_entries = (consecutive_down >= window[0])\n",
    "    time_mask = (df.index.hour < 15)\n",
    "    final_long_entries = long_entries & time_mask\n",
    "    final_short_entries = short_entries & time_mask\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=final_long_entries, short_entries=final_short_entries,\n",
    "        sl_stop=sl_stop[0], sl_trail=True, freq='1min', init_cash=INITIAL_CASH, fees=0.0001,\n",
    "        slippage=0.0002, size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "@njit\n",
    "def _expanding_profile_nb(a: np.ndarray, phi_va: float, min_periods: int) -> np.ndarray:\n",
    "    output = np.full((a.shape[0], 3), np.nan, dtype=np.float64)\n",
    "    close_col, volume_col = a[:, 0], a[:, 1]\n",
    "    for i in range(a.shape[0]):\n",
    "        if i + 1 >= min_periods:\n",
    "            current_close_window, current_volume_window = close_col[:i+1], volume_col[:i+1]\n",
    "            poc, vah, val = np.nan, np.nan, np.nan\n",
    "            valid_mask = ~np.isnan(current_close_window) & ~np.isnan(current_volume_window)\n",
    "            close, volume = current_close_window[valid_mask], current_volume_window[valid_mask]\n",
    "            if close.shape[0] > 0:\n",
    "                price_bins = np.floor(close * 100) / 100\n",
    "                unique_prices = np.unique(price_bins)\n",
    "                daily_profile = np.zeros(len(unique_prices), dtype=np.float64)\n",
    "                for k in range(len(price_bins)):\n",
    "                    idx = np.searchsorted(unique_prices, price_bins[k])\n",
    "                    if idx < len(unique_prices) and unique_prices[idx] == price_bins[k]:\n",
    "                        daily_profile[idx] += volume[k]\n",
    "                total_volume = np.sum(daily_profile)\n",
    "                if total_volume > 0:\n",
    "                    max_volume_indices = np.where(daily_profile == np.max(daily_profile))[0]\n",
    "                    poc_idx = max_volume_indices[np.argmin(np.abs(unique_prices[max_volume_indices] - (np.max(unique_prices) + np.min(unique_prices)) / 2))]\n",
    "                    poc = unique_prices[poc_idx]\n",
    "                    target_volume = total_volume * phi_va\n",
    "                    va_indices, va_volume = [poc_idx], daily_profile[poc_idx]\n",
    "                    up_idx, down_idx = poc_idx + 1, poc_idx - 1\n",
    "                    while va_volume < target_volume and (up_idx < len(daily_profile) or down_idx >= 0):\n",
    "                        vol_above = daily_profile[up_idx] if up_idx < len(daily_profile) else -1.0\n",
    "                        vol_below = daily_profile[down_idx] if down_idx >= 0 else -1.0\n",
    "                        if vol_above > vol_below:\n",
    "                            va_indices.append(up_idx)\n",
    "                            va_volume += vol_above\n",
    "                            up_idx += 1\n",
    "                        else:\n",
    "                            va_indices.append(down_idx)\n",
    "                            va_volume += vol_below\n",
    "                            down_idx -= 1\n",
    "                    value_area_prices = unique_prices[np.array(va_indices, dtype=np.int64)]\n",
    "                    vah, val = np.max(value_area_prices), np.min(value_area_prices)\n",
    "            output[i, 0], output[i, 1], output[i, 2] = poc, vah, val\n",
    "    return output\n",
    "\n",
    "def calculate_developing_profiles(df: pd.DataFrame, phi_va: float, min_periods: int = 60) -> pd.DataFrame:\n",
    "    def _apply_kernel_to_day(day_df: pd.DataFrame, phi_va: float, min_p: int) -> pd.DataFrame:\n",
    "        metrics = _expanding_profile_nb(day_df[['close', 'volume']].values, phi_va, min_p)\n",
    "        return pd.DataFrame(metrics, index=day_df.index, columns=['poc', 'vah', 'val'])\n",
    "    daily_results = df.groupby(df.index.normalize()).apply(lambda x: _apply_kernel_to_day(x, phi_va, min_periods))\n",
    "    daily_results.index = daily_results.index.droplevel(0)\n",
    "    return daily_results\n",
    "\n",
    "def run_volume_breakout(df: pd.DataFrame, phi_va: list, kappa_surge: list, timeperiod: list, adx_threshold: list, alpha_atr: list, alpha_tp: list, **kwargs):\n",
    "    vp = calculate_developing_profiles(df, phi_va[0])\n",
    "    vah, val = vp['vah'], vp['val']\n",
    "    adx = vbt.talib('ADX').run(df['high'], df['low'], df['close'], timeperiod=timeperiod[0]).real\n",
    "    volume_confirm = (df['volume'] > kappa_surge[0] * df['volume_avg'])\n",
    "    adx_confirm = (adx > adx_threshold[0])\n",
    "    long_entries = (df['close'] > vah) & volume_confirm & adx_confirm\n",
    "    short_entries = (df['close'] < val) & volume_confirm & adx_confirm\n",
    "    time_mask = (df.index.hour < 15)\n",
    "    final_long_entries, final_short_entries = long_entries & time_mask, short_entries & time_mask\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    tp_pct = (alpha_tp[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=final_long_entries, short_entries=final_short_entries,\n",
    "        sl_stop=sl_pct, tp_stop=tp_pct, freq='1min', init_cash=INITIAL_CASH, fees=0.0001,\n",
    "        slippage=0.0002, size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_volume_momentum(df: pd.DataFrame, timeperiod: list, kappa_vol_mom: list, adx_threshold: list, alpha_atr: list, **kwargs):\n",
    "    adx = vbt.talib('ADX').run(df['high'], df['low'], df['close'], timeperiod=timeperiod[0]).real\n",
    "    adx_confirm = adx > adx_threshold[0]\n",
    "    rel_vol_confirm = df['relative_volume'] > kappa_vol_mom[0]\n",
    "    long_entries = (df['close'] > df['prev_session_high']) & rel_vol_confirm & adx_confirm\n",
    "    short_entries = (df['close'] < df['prev_session_low']) & rel_vol_confirm & adx_confirm\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=long_entries, short_entries=short_entries, sl_stop=sl_pct,\n",
    "        sl_trail=True, freq='1min', init_cash=INITIAL_CASH, fees=0.0001, slippage=0.0002,\n",
    "        size=0.1, size_type=SizeType.Percent, upon_long_conflict='ignore',\n",
    "        upon_opposite_entry='ignore', reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_volume_vwap_reversion(df: pd.DataFrame, window: list, quantile: list, slope: list, tau_vwap_trend: list, alpha_atr: list, alpha_tp: list, **kwargs):\n",
    "    vwap_trend = df['vwap_actual'].shift(tau_vwap_trend[0])\n",
    "    obv_lower_q = df['obv'].rolling(window=window[0]).quantile(quantile[0])\n",
    "    obv_upper_q = df['obv'].rolling(window=window[0]).quantile(1.0 - quantile[0])\n",
    "    trend_down, trend_up = df['vwap_actual'] < vwap_trend, df['vwap_actual'] > vwap_trend\n",
    "    long_entries = (df['close'] < df['vwap_actual']) & (df['obv'] < obv_lower_q) & (df['vwap_actual_slope'] >= slope[0]) & trend_down\n",
    "    short_entries = (df['close'] > df['vwap_actual']) & (df['obv'] > obv_upper_q) & (df['vwap_actual_slope'] <= -slope[0]) & trend_up\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    tp_pct = (alpha_tp[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=long_entries, short_entries=short_entries, sl_stop=sl_pct,\n",
    "        tp_stop=tp_pct, freq='1min', init_cash=INITIAL_CASH, fees=0.0001, slippage=0.0002,\n",
    "        size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_dl_breakout(df: pd.DataFrame, phi_va: list, kappa_dl: list, timeperiod: list, adx_threshold: list, alpha_atr: list, alpha_tp: list, **kwargs):\n",
    "    profile_df = df[['close']].copy()\n",
    "    profile_df['volume'] = df['pred_volume_15_tft']\n",
    "    vp = calculate_developing_profiles(profile_df, phi_va[0])\n",
    "    vah, val = vp['vah'], vp['val']\n",
    "    adx = vbt.talib('ADX').run(df['high'], df['low'], df['close'], timeperiod=timeperiod[0]).real\n",
    "    volume_confirm = (df['pred_volume_15_tft'] > kappa_dl[0] * df['volume_avg'])\n",
    "    adx_confirm = (adx > adx_threshold[0])\n",
    "    long_entries = (df['close'] > vah) & volume_confirm & adx_confirm\n",
    "    short_entries = (df['close'] < val) & volume_confirm & adx_confirm\n",
    "    time_mask = (df.index.hour < 15)\n",
    "    final_long_entries, final_short_entries = long_entries & time_mask, short_entries & time_mask\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    tp_pct = (alpha_tp[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=final_long_entries, short_entries=final_short_entries,\n",
    "        sl_stop=sl_pct, tp_stop=tp_pct, freq='1min', init_cash=INITIAL_CASH, fees=0.0001,\n",
    "        slippage=0.0002, size=0.1, size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_dl_volume_momentum(df: pd.DataFrame, timeperiod: list, kappa_dl: list, adx_threshold: list, alpha_atr: list, tau_vol_trend: list, **kwargs):\n",
    "    adx = vbt.talib('ADX').run(df['high'], df['low'], df['close'], timeperiod=timeperiod[0]).real\n",
    "    vol_trend = df['pred_volume_tft_15_scaled'].rolling(window=tau_vol_trend[0]).mean()\n",
    "    adx_confirm = adx > adx_threshold[0]\n",
    "    volume_confirm = vol_trend > kappa_dl[0] * df['volume_avg']\n",
    "    long_entries = (df['close'] > df['prev_session_high']) & volume_confirm & adx_confirm\n",
    "    short_entries = (df['close'] < df['prev_session_low']) & volume_confirm & adx_confirm\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=long_entries, short_entries=short_entries, sl_stop=sl_pct,\n",
    "        sl_trail=True, freq='1min', init_cash=INITIAL_CASH, fees=0.0001, slippage=0.0002,\n",
    "        size=0.1, size_type=SizeType.Percent, upon_long_conflict='ignore',\n",
    "        upon_opposite_entry='ignore', reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_dl_vwap_reversion(df: pd.DataFrame, delta_vwap: list, tau_vwap_trend: list, volume_multiplier: list, alpha_atr: list, alpha_tp: list, **kwargs):\n",
    "    vwap_trend = df['vwap_dl'].shift(tau_vwap_trend[0])\n",
    "    trend_down, trend_up = df['vwap_dl'] < vwap_trend, df['vwap_dl'] > vwap_trend\n",
    "    pred_vol_high = df['pred_volume_tft_15_scaled'] > (volume_multiplier[0] * df['volume'])\n",
    "    price_below_vwap = df['close'] < (1 - delta_vwap[0]) * df['vwap_dl']\n",
    "    price_above_vwap = df['close'] > (1 + delta_vwap[0]) * df['vwap_dl']\n",
    "    long_entries = price_below_vwap & trend_down & pred_vol_high\n",
    "    short_entries = price_above_vwap & trend_up & pred_vol_high\n",
    "    long_exits = df['close'].vbt.crossed_above(df['vwap_dl'])\n",
    "    short_exits = df['close'].vbt.crossed_below(df['vwap_dl'])\n",
    "    sl_pct = (alpha_atr[0] * df['atr']) / df['close']\n",
    "    tp_pct = (alpha_tp[0] * df['atr']) / df['close']\n",
    "    return vbt.Portfolio.from_signals(\n",
    "        df['close'], entries=long_entries, exits=long_exits, short_entries=short_entries,\n",
    "        short_exits=short_exits, sl_stop=sl_pct, tp_stop=tp_pct, freq='1min',\n",
    "        init_cash=INITIAL_CASH, fees=0.0001, slippage=0.0002, size=0.1,\n",
    "        size_type=SizeType.Percent, upon_opposite_entry='ignore',\n",
    "        reject_prob=REJECT_PROBABILITY, allow_partial=True, **kwargs\n",
    "    )\n",
    "\n",
    "def run_and_evaluate_strategy(strategy_name: str, ticker: str, strategy_func, df: pd.DataFrame, params: Dict) -> Optional[pd.Series]:\n",
    "    \"\"\"\n",
    "    Runs a single strategy simulation, prints a report, and saves all relevant\n",
    "    trade, order, position, log, and time-series data to CSV files.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"--- Running Simulation for: {strategy_name} on Ticker: {ticker} ---\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(f\"Skipping '{strategy_name}' for {ticker} due to missing data.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        single_value_params = {k: v[0] for k, v in params.items()}\n",
    "\n",
    "        print(\"DEBUG: Running strategy function...\")\n",
    "        pf = strategy_func(df, **params, log=True)\n",
    "        print(f\"DEBUG: Portfolio object created. Type: {type(pf)}, Shape: {pf.wrapper.shape}\")\n",
    "\n",
    "  \n",
    "        if pf.wrapper.ndim > 1 and pf.wrapper.shape[1] > 1:\n",
    "            print(f\"DEBUG: Portfolio has multiple columns ({pf.wrapper.shape[1]}). Selecting first column for stats and data saving.\")\n",
    "            pf_single_col = pf.iloc[:, 0]\n",
    "        else:\n",
    "            print(\"DEBUG: Portfolio has a single column.\")\n",
    "            pf_single_col = pf\n",
    "\n",
    "        print(\"\\n--- Strategy Parameters ---\")\n",
    "        print(pd.Series(single_value_params))\n",
    "\n",
    "        print(\"\\n--- Performance Summary ---\")\n",
    "        all_metrics = list(pf_single_col.metrics.items())\n",
    "\n",
    "        try:\n",
    "            import quantstats as qs\n",
    "            kelly_metric = ('Kelly Criterion', dict(\n",
    "                title='Kelly Criterion',\n",
    "                calc_func=lambda returns: qs.stats.kelly_criterion(returns), \n",
    "                tags='risk'\n",
    "            ))\n",
    "            all_metrics.append(kelly_metric)\n",
    "        except ImportError:\n",
    "            print(\"Warning: `quantstats` not installed. Skipping Kelly Criterion calculation.\")\n",
    "\n",
    "\n",
    "        print(\"DEBUG: Calculating stats...\")\n",
    "        stats = pf_single_col.stats(metrics=all_metrics)\n",
    "        print(\"DEBUG: Stats calculation complete.\")\n",
    "        print(stats)\n",
    "\n",
    "        # Saving all simulation data\n",
    "        print(\"\\n--- Saving All Simulation Data ---\")\n",
    "\n",
    "        results_dir = os.path.join(BASE_DIR, \"simulation_results\")\n",
    "        ticker_dir = os.path.join(results_dir, ticker)\n",
    "        strategy_dir = os.path.join(ticker_dir, strategy_name.replace(' ', '_'))\n",
    "        os.makedirs(strategy_dir, exist_ok=True)\n",
    "\n",
    "        def save_data(data_to_save, data_name, s_dir):\n",
    "            \"\"\"Helper function to save DataFrame or Series to a CSV file.\"\"\"\n",
    "            if data_to_save is not None and not data_to_save.empty:\n",
    "                filename = os.path.join(s_dir, f\"{data_name}.csv\")\n",
    "                index_to_save = isinstance(data_to_save.index, pd.DatetimeIndex)\n",
    "                if isinstance(data_to_save, pd.Series):\n",
    "                    data_to_save.to_csv(filename, index=True, header=True)\n",
    "                else:\n",
    "                    data_to_save.to_csv(filename, index=index_to_save)\n",
    "                print(f\"    - Saved {data_name} data to {filename}\")\n",
    "            else:\n",
    "                print(f\"    - No data to save for {data_name}\")\n",
    "\n",
    "        print(\"DEBUG: Saving orders, trades, and positions...\")\n",
    "        save_data(pf_single_col.orders.records_readable, 'orders', strategy_dir)\n",
    "        save_data(pf_single_col.trades.records_readable, 'trades', strategy_dir)\n",
    "        save_data(pf_single_col.positions.records_readable, 'positions', strategy_dir)\n",
    "\n",
    "        print(\"DEBUG: Checking and saving logs...\")\n",
    "        if pf_single_col.logs is not None and not pf_single_col.logs.records_readable.empty:\n",
    "            save_data(pf_single_col.logs.records_readable, 'logs', strategy_dir)\n",
    "        else:\n",
    "            print(\"    - No data to save for logs\")\n",
    "\n",
    "        print(\"DEBUG: Creating and saving timeseries data...\")\n",
    "        timeseries_df = pd.DataFrame({\n",
    "            'portfolio_value': pf_single_col.value().squeeze(),\n",
    "            'asset_value': pf_single_col.asset_value().squeeze(),\n",
    "            'cash': pf_single_col.cash().squeeze(),\n",
    "            'gross_exposure': pf_single_col.gross_exposure().squeeze(),\n",
    "            'net_exposure': pf_single_col.net_exposure().squeeze()\n",
    "        }, index=pf_single_col.wrapper.index)\n",
    "        save_data(timeseries_df, 'portfolio_timeseries', strategy_dir)\n",
    "\n",
    "        print(\"DEBUG: Saving drawdown data...\")\n",
    "        save_data(pf_single_col.drawdowns.records_readable, 'drawdowns', strategy_dir)\n",
    "\n",
    "        print(f\"\\n--- Finished Simulation for: {strategy_name} ---\")\n",
    "        return stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! AN ERROR OCCURRED during simulation of '{strategy_name}' on {ticker}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    \"\"\"Main function to run simulations for all strategies and tickers.\"\"\"\n",
    "    try:\n",
    "        with open(METADATA_FILE, 'r') as f:\n",
    "            tickers = [item['Ticker'] for item in json.load(f)]\n",
    "        tickers_to_process = tickers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Metadata file not found at {METADATA_FILE}\")\n",
    "        return\n",
    "\n",
    "    results_dir = os.path.join(BASE_DIR, \"simulation_results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    for ticker in tickers_to_process:\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"##### Processing Ticker: {ticker} #####\")\n",
    "        print(f\"{'#'*80}\")\n",
    "\n",
    "        df_baseline = load_simulation_data(ticker, 'Baseline')\n",
    "        df_volume = load_simulation_data(ticker, 'Volume_Enhanced')\n",
    "        df_dl = load_simulation_data(ticker, 'Deep_Learning_Enhanced')\n",
    "\n",
    "        simulation_tasks = {\n",
    "            \"Baseline Breakout\": (run_baseline_breakout, df_baseline, BEST_BASELINE_BREAKOUT_PARAMS),\n",
    "            \"Baseline Momentum\": (run_baseline_momentum, df_baseline, BEST_BASELINE_MOMENTUM_PARAMS),\n",
    "            \"Baseline Bollinger Bands\": (run_baseline_bbands, df_baseline, BEST_BASELINE_BBANDS_PARAMS),\n",
    "            \"Volume-Enhanced Breakout\": (run_volume_breakout, df_volume, BEST_VOLUME_BREAKOUT_PARAMS),\n",
    "            \"Volume-Enhanced VWAP Reversion\": (run_volume_vwap_reversion, df_volume, BEST_VOLUME_VWAP_REVERSION_PARAMS),\n",
    "            \"Volume-Enhanced Momentum\": (run_volume_momentum, df_volume, BEST_VOLUME_MOMENTUM_PARAMS),\n",
    "            \"Deep Learning Breakout\": (run_dl_breakout, df_dl, BEST_DL_BREAKOUT_PARAMS),\n",
    "            \"Deep Learning VWAP Reversion\": (run_dl_vwap_reversion, df_dl, BEST_DL_VWAP_REVERSION_PARAMS),\n",
    "            \"Deep Learning Momentum\": (run_dl_volume_momentum, df_dl, BEST_DL_VOLUME_MOMENTUM_PARAMS),\n",
    "        }\n",
    "\n",
    "        for name, (func, df, params) in simulation_tasks.items():\n",
    "            stats = run_and_evaluate_strategy(name, ticker, func, df, params)\n",
    "            if stats is not None:\n",
    "                stats_df = stats.to_frame(name=ticker).T\n",
    "                stats_df['Strategy'] = name\n",
    "                stats_df['Ticker'] = ticker\n",
    "                for p_name, p_val in params.items():\n",
    "                    stats_df[p_name] = p_val[0]\n",
    "                all_results.append(stats_df)\n",
    "\n",
    "    if all_results:\n",
    "        print(\"\\n--- Aggregating all simulation results ---\")\n",
    "        final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        all_param_keys = set()\n",
    "        for d in [BEST_BASELINE_BREAKOUT_PARAMS, BEST_BASELINE_BBANDS_PARAMS, BEST_BASELINE_MOMENTUM_PARAMS,\n",
    "                  BEST_VOLUME_BREAKOUT_PARAMS, BEST_VOLUME_MOMENTUM_PARAMS, BEST_VOLUME_VWAP_REVERSION_PARAMS,\n",
    "                  BEST_DL_BREAKOUT_PARAMS, BEST_DL_VOLUME_MOMENTUM_PARAMS, BEST_DL_VWAP_REVERSION_PARAMS]:\n",
    "            all_param_keys.update(d.keys())\n",
    "        \n",
    "        metric_cols = [col for col in final_results_df.columns if col not in ['Ticker', 'Strategy'] and col not in all_param_keys]\n",
    "        param_cols = [col for col in final_results_df.columns if col in all_param_keys]\n",
    "        \n",
    "        final_cols = ['Ticker', 'Strategy'] + sorted(list(param_cols)) + metric_cols\n",
    "        final_results_df = final_results_df.reindex(columns=final_cols)\n",
    "        \n",
    "        output_filename = os.path.join(results_dir, \"simulation_summary.csv\")\n",
    "        final_results_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nSuccessfully saved comprehensive simulation summary to {output_filename}\")\n",
    "        print(final_results_df)\n",
    "    else:\n",
    "        print(\"\\nNo successful simulations to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df482f6-9ee0-4eb1-b3ad-c9dd81355321",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
